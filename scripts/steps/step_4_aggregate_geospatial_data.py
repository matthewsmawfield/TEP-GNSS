#!/usr/bin/env python3
"""
TEP GNSS Analysis - STEP 4: Geospatial Data Processing
======================================================

Processes correlation analysis output from Step 3 and enriches station-pair
data with geospatial metrics for advanced statistical analysis and visualization.

Requirements: Step 3 complete
Next: Step 5 (Statistical Validation)

Algorithm Overview:
1.  Scan the `results/tmp/` directory for all step_3_pairs_*.csv files
    generated by Step 3.
2.  Group these files by their analysis center (e.g., 'igs', 'code', 'esa').
3.  For each analysis center:
    a. Concatenate all corresponding pair-data CSVs into a single DataFrame.
    b. For each station pair in the DataFrame, calculate additional geospatial metrics:
        - Azimuth (bearing from station 1 to station 2).
        - Delta Longitude (absolute difference in longitude).
        - Delta Local Time (time difference based on longitude).
    c. Save the enriched DataFrame to a new, aggregated CSV file in the
       `data/processed/` directory.

This approach ensures that downstream scripts have access to all necessary data
for advanced anisotropy and diurnal effect analysis without needing to touch the
core correlation pipeline.

Inputs:
  - `results/tmp/step_3_pairs_*.csv`

Outputs:
  - `data/processed/step_4_geospatial_{ac}.csv`

Author: Matthew Lukin Smawfield
Date: September 2025
"""

import os
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from glob import glob
import re

# Anchor to package root
ROOT = Path(__file__).resolve().parents[2]

# Import TEP utilities for better configuration and error handling
import sys
sys.path.insert(0, str(ROOT))
from scripts.utils.config import TEPConfig
from scripts.utils.exceptions import (
    SafeErrorHandler, TEPDataError, TEPFileError, 
    safe_csv_read, safe_json_write, validate_directory_exists
)

def print_status(text: str, status: str = "INFO"):
    """Print verbose status message with timestamp"""
    from datetime import datetime
    timestamp = datetime.now().strftime("%H:%M:%S")
    prefixes = {"INFO": "[INFO]", "SUCCESS": "[SUCCESS]", "WARNING": "[WARNING]", "ERROR": "[ERROR]", "PROCESS": "[PROCESSING]"}
    print(f"{timestamp} {prefixes.get(status, '[INFO]')} {text}")

def compute_azimuth(lat1, lon1, lat2, lon2):
    """
    Compute azimuth (bearing) from station 1 to station 2.
    Returns azimuth in degrees (0-360, where 0=North, 90=East).
    This function is adapted from step_3_tep_correlation_analysis.py.
    """
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    
    dlon = lon2 - lon1
    y = np.sin(dlon) * np.cos(lat2)
    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)
    
    azimuth = np.arctan2(y, x)
    azimuth = np.degrees(azimuth)
    azimuth = (azimuth + 360) % 360  # Normalize to 0-360
    
    return azimuth

def main():
    """Main function to find, aggregate, and enrich pair-level data."""
    print("\n" + "="*80)
    print("TEP GNSS Analysis - STEP 4: Geospatial Data Processing")
    print("Enriching correlation data with spatial metrics")
    print("="*80)

    pair_data_dir = ROOT / "results/tmp"
    output_dir = ROOT / "data/processed"
    output_dir.mkdir(parents=True, exist_ok=True)

    if not pair_data_dir.exists():
        print_status(f"Pair data directory not found: {pair_data_dir}", "ERROR")
        print_status("Please run Step 3 with TEP_WRITE_PAIR_LEVEL=1 to generate the required files.", "ERROR")
        return False

    all_pair_files = glob(str(pair_data_dir / "step_3_pairs_*.csv"))

    if not all_pair_files:
        print_status("No pair-level CSV files found.", "ERROR")
        print_status("Please run Step 3 with TEP_WRITE_PAIR_LEVEL=1 to generate the required files.", "ERROR")
        return False

    print_status(f"Found {len(all_pair_files)} pair-level data files to process.", "INFO")

    # Group files by analysis center
    analysis_centers = {}
    for f in all_pair_files:
        match = re.search(r'pairs_(igs_combined|esa_final|code)_', Path(f).name)
        if match:
            ac = match.group(1)
            if ac not in analysis_centers:
                analysis_centers[ac] = []
            analysis_centers[ac].append(f)

    for ac, files in analysis_centers.items():
        print_status(f"Processing analysis center: {ac.upper()} ({len(files)} files)", "PROCESS")

        # 1. Concatenate all CSVs for the center using safe operations
        df_chunks = []
        for f in files:
            try:
                chunk = safe_csv_read(f)
                if chunk is not None:
                    df_chunks.append(chunk)
            except (TEPDataError, TEPFileError) as e:
                print_status(f"Failed to load {f}: {e}", "WARNING")
                continue
        
        if not df_chunks:
            print_status(f"No valid files found for {ac.upper()}. Skipping.", "WARNING")
            continue
            
        df = pd.concat(df_chunks, ignore_index=True)

        # Drop rows where coordinate data is missing
        coord_cols = ['station1_lat', 'station1_lon', 'station2_lat', 'station2_lon']
        df.dropna(subset=coord_cols, inplace=True)
        if df.empty:
            print_status(f"No valid coordinate data found for {ac.upper()}. Skipping.", "WARNING")
            continue

        # 2. Calculate new geospatial metrics
        print_status("Calculating azimuth...", "INFO")
        df['azimuth'] = compute_azimuth(
            df['station1_lat'], df['station1_lon'],
            df['station2_lat'], df['station2_lon']
        )

        print_status("Calculating longitude and local time differences...", "INFO")
        # Absolute difference in longitude, handling wrap-around at 180 degrees
        lon_diff = np.abs(df['station2_lon'] - df['station1_lon'])
        df['delta_longitude'] = np.minimum(lon_diff, 360 - lon_diff)

        # Local time difference in hours (15 degrees = 1 hour)
        df['delta_local_time'] = df['delta_longitude'] / 15.0

        # 3. Save the enriched DataFrame
        output_filename = output_dir / f"step_4_geospatial_{ac}.csv"
        df.to_csv(output_filename, index=False)
        print_status(f"Saved enriched data for {ac.upper()} to {output_filename}", "SUCCESS")
        print_status(f"DataFrame shape: {df.shape}", "SUCCESS")

    print("\n" + "="*80)
    print("STEP 4 COMPLETE")
    print("="*80)
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
