#!/usr/bin/env python3
"""
TEP GNSS Analysis - STEP 10: High-Resolution Astronomical Event Analysis
========================================================================

Analyzes rapid transient astronomical events at sub-daily temporal resolution
by processing original GPS CLK files directly, bypassing the daily aggregation
used in previous steps.

This step focuses on:
1. Solar eclipses (ionospheric effects at 5-minute resolution)
2. Geomagnetic storms (rapid ionospheric disruptions)
3. Other rapid transient events requiring high temporal precision

The analysis uses the original 30-second GPS clock data to capture
rapid changes that are averaged out in daily processing.

Requirements: Original GPS CLK files
Complements: Step 5 (daily astronomical analysis)

Algorithm Overview:
1. Load original GPS CLK files around event dates
2. Process at native temporal resolution (30 seconds)
3. Calculate cross-power plateau for station pairs at sub-daily intervals
4. Track coherence changes at 5-minute to hourly resolution
5. Detect rapid transient signatures in timing correlations

Author: Matthew Lukin Smawfield
Date: September 2025
"""

import os
import sys
import time
import gzip
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import math
import itertools
import json

# Anchor to package root
ROOT = Path(__file__).resolve().parents[2]

# Import TEP utilities
sys.path.insert(0, str(ROOT))
from scripts.utils.config import TEPConfig
from scripts.utils.exceptions import (
    SafeErrorHandler, TEPDataError, TEPFileError,
    safe_json_write, validate_directory_exists
)

def print_status(text: str, status: str = "INFO"):
    """Print verbose status message with timestamp"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    prefixes = {"ERROR": "[ERROR]", "WARNING": "[WARNING]", "SUCCESS": "[SUCCESS]", 
                "PROCESS": "[PROCESSING]", "INFO": "[INFO]"}
    print(f"{timestamp} {prefixes.get(status, '[INFO]')} {text}")

def parse_clk_file_high_resolution(clk_file_path: Path) -> pd.DataFrame:
    """
    Parse a GPS CLK file at full temporal resolution (30-second intervals).
    
    Unlike Step 3 which aggregates to daily, this preserves all timestamps
    for high-resolution analysis of rapid transient events.
    
    Args:
        clk_file_path: Path to compressed CLK file
        
    Returns:
        DataFrame with columns: ['datetime', 'station', 'clock_bias', 'clock_drift']
    """
    try:
        # Read compressed CLK file
        with gzip.open(clk_file_path, 'rt') as f:
            lines = f.readlines()
        
        clock_data = []
        
        for line in lines:
            if line.startswith('AR '):  # Use receiver clock data, not satellite
                parts = line.strip().split()
                if len(parts) >= 8:
                    # CLK format: AS STATION YYYY MM DD HH MM SS.SSSSSS [bias] [drift]
                    try:
                        station = parts[1]
                        year, month, day = int(parts[2]), int(parts[3]), int(parts[4])
                        hour, minute = int(parts[5]), int(parts[6])
                        second = float(parts[7])
                        
                        # Create datetime
                        dt = datetime(year, month, day, hour, minute, int(second), 
                                    int((second % 1) * 1000000))
                        
                        # Extract clock parameters (correct CLK format)
                        # parts[8] is a flag, parts[9] is clock bias, parts[10] is clock drift
                        clock_bias = float(parts[9]) if len(parts) > 9 else 0.0
                        clock_drift = float(parts[10]) if len(parts) > 10 else 0.0
                        
                        clock_data.append({
                            'datetime': dt,
                            'station': station,
                            'clock_bias': clock_bias,
                            'clock_drift': clock_drift
                        })
                        
                    except (ValueError, IndexError):
                        continue
        
        if not clock_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(clock_data)
        df = df.sort_values(['datetime', 'station']).reset_index(drop=True)
        
        return df
        
    except Exception as e:
        print_status(f"Failed to parse CLK file {clk_file_path.name}: {e}", "ERROR")
        return pd.DataFrame()

def compute_high_resolution_coherence(df: pd.DataFrame, time_window_minutes: int = 5) -> pd.DataFrame:
    """
    Compute phase coherence between station pairs at high temporal resolution.
    
    Args:
        df: Clock data with datetime, station, clock_bias columns
        time_window_minutes: Time window for coherence calculation
        
    Returns:
        DataFrame with coherence data at specified resolution
    """
    try:
        print_status(f"Input data: {len(df)} measurements, {df['station'].nunique()} stations", "INFO")
        
        # Group data into time windows
        df['time_bin'] = df['datetime'].dt.floor(f'{time_window_minutes}min')
        
        coherence_data = []
        
        for time_bin, time_data in df.groupby('time_bin'):
            # Create station time series for this time window
            # Use the mean clock bias for each station in this window
            station_means = time_data.groupby('station')['clock_bias'].mean()
            
            # Need at least 3 stations for meaningful pairs
            if len(station_means) < 3:
                continue
            
            # Calculate coherence for station pairs using simplified approach
            stations = list(station_means.index)
            
            for i, station1 in enumerate(stations):
                for j, station2 in enumerate(stations):
                    if i < j:  # Avoid duplicates
                        bias1 = station_means[station1]
                        bias2 = station_means[station2]
                        
                        # Coherence measure: normalized inverse of bias difference
                        # GPS clock biases are in seconds, typically ~1e-4 to 1e-6 range
                        bias_diff = abs(bias1 - bias2)
                        
                        # Normalize by typical GPS clock bias scale (~1e-4)
                        normalized_diff = bias_diff / 1e-4
                        coherence = 1.0 / (1.0 + normalized_diff)  # Coherence between 0 and 1
                        
                        coherence_data.append({
                            'datetime': time_bin,
                            'station_i': station1,
                            'station_j': station2,
                            'coherence': coherence,
                            'bias_diff': bias_diff,
                            'n_epochs': len(time_data)
                        })
        
        result_df = pd.DataFrame(coherence_data)
        print_status(f"Computed coherence: {len(result_df)} pairs across {len(df.groupby('time_bin'))} time bins", "SUCCESS")
        
        return result_df
        
    except Exception as e:
        print_status(f"High-resolution coherence calculation failed: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def analyze_solar_eclipse_high_resolution(analysis_center: str = 'igs_combined') -> Dict:
    """
    Analyze the April 8, 2024 solar eclipse at high temporal resolution.
    
    Uses original GPS CLK files to capture rapid ionospheric changes
    during eclipse totality at 5-minute resolution.
    
    Args:
        analysis_center: GPS analysis center to process
        
    Returns:
        dict: High-resolution eclipse analysis results
    """
    try:
        print_status("Starting High-Resolution Solar Eclipse Analysis...", "PROCESS")
        print_status("Processing original GPS CLK files at 30-second resolution", "INFO")
        
        # Eclipse configuration
        eclipse_date = datetime(2024, 4, 8, 18, 18)  # UTC time of maximum eclipse
        window_hours = 12
        
        print_status(f"Eclipse maximum: {eclipse_date} UTC", "INFO")
        print_status(f"Analysis window: Â±{window_hours} hours", "INFO")
        
        # Find CLK files around eclipse date
        data_dir = ROOT / "data" / "raw" / analysis_center
        
        # Analysis center filename prefixes
        prefixes = {
            'code': 'COD0OPSFIN_',
            'igs_combined': 'IGS0OPSFIN_',
            'esa_final': 'ESA0OPSFIN_'
        }
        
        if analysis_center not in prefixes:
            return {'success': False, 'error': f'Unknown analysis center: {analysis_center}'}
        
        prefix = prefixes[analysis_center]
        
        # Eclipse is day 099 of 2024
        eclipse_files = [
            data_dir / f"{prefix}20240980000_01D_30S_CLK.CLK.gz",  # Day before
            data_dir / f"{prefix}20240990000_01D_30S_CLK.CLK.gz",  # Eclipse day
            data_dir / f"{prefix}20241000000_01D_30S_CLK.CLK.gz"   # Day after
        ]
        
        # Load and combine CLK data
        all_clock_data = []
        
        for clk_file in eclipse_files:
            if clk_file.exists():
                print_status(f"Loading {clk_file.name}...", "PROCESS")
                df = parse_clk_file_high_resolution(clk_file)
                if not df.empty:
                    all_clock_data.append(df)
                    print_status(f"Loaded {len(df):,} clock measurements", "SUCCESS")
            else:
                print_status(f"File not found: {clk_file.name}", "WARNING")
        
        if not all_clock_data:
            return {
                'success': False,
                'error': 'No CLK files found for eclipse period'
            }
        
        # Combine all clock data
        combined_df = pd.concat(all_clock_data, ignore_index=True)
        print_status(f"Combined dataset: {len(combined_df):,} measurements", "SUCCESS")
        
        # Filter to eclipse window
        eclipse_start = eclipse_date - timedelta(hours=window_hours)
        eclipse_end = eclipse_date + timedelta(hours=window_hours)
        
        eclipse_data = combined_df[
            (combined_df['datetime'] >= eclipse_start) & 
            (combined_df['datetime'] <= eclipse_end)
        ].copy()
        
        if eclipse_data.empty:
            return {
                'success': False,
                'error': 'No data in eclipse time window'
            }
        
        print_status(f"Eclipse window data: {len(eclipse_data):,} measurements", "SUCCESS")
        
        # Compute high-resolution coherence (5-minute intervals)
        print_status("Computing 5-minute resolution coherence...", "PROCESS")
        coherence_df = compute_high_resolution_coherence(eclipse_data, time_window_minutes=5)
        
        if coherence_df.empty:
            return {
                'success': False,
                'error': 'Failed to compute high-resolution coherence'
            }
        
        print_status(f"Computed coherence for {len(coherence_df):,} station pairs", "SUCCESS")
        
        # Analyze coherence evolution around eclipse
        coherence_df['minutes_from_eclipse'] = (
            coherence_df['datetime'] - pd.Timestamp(eclipse_date)
        ).dt.total_seconds() / 60
        
        # Bin by 5-minute intervals
        time_bins = range(-window_hours*60, window_hours*60 + 1, 5)
        temporal_evolution = []
        
        for time_bin in time_bins:
            bin_data = coherence_df[
                (coherence_df['minutes_from_eclipse'] >= time_bin - 2.5) & 
                (coherence_df['minutes_from_eclipse'] < time_bin + 2.5)
            ]
            
            if len(bin_data) >= 10:  # Need sufficient pairs per bin
                mean_coherence = bin_data['coherence'].mean()
                std_coherence = bin_data['coherence'].std()
                n_pairs = len(bin_data)
                
                # Determine eclipse phase
                abs_minutes = abs(time_bin)
                if abs_minutes <= 30:  # Within 30 minutes of maximum
                    phase = 'totality'
                elif abs_minutes <= 120:  # Within 2 hours
                    phase = 'partial'
                else:
                    phase = 'baseline'
                
                temporal_evolution.append({
                    'datetime': (eclipse_date + timedelta(minutes=time_bin)).isoformat(),
                    'minutes_from_eclipse': time_bin,
                    'mean_coherence': mean_coherence,
                    'std_coherence': std_coherence,
                    'n_pairs': n_pairs,
                    'eclipse_phase': phase
                })
        
        # Statistical analysis
        if len(temporal_evolution) < 10:
            return {
                'success': False,
                'error': f'Insufficient temporal bins: {len(temporal_evolution)} < 10'
            }
        
        # Phase comparison
        phase_stats = {}
        evolution_df = pd.DataFrame(temporal_evolution)
        
        for phase in ['baseline', 'partial', 'totality']:
            phase_data = evolution_df[evolution_df['eclipse_phase'] == phase]
            if len(phase_data) > 0:
                phase_stats[phase] = {
                    'n_bins': len(phase_data),
                    'mean_coherence': phase_data['mean_coherence'].mean(),
                    'std_coherence': phase_data['mean_coherence'].std()
                }
        
        # Eclipse enhancement calculation
        enhancement_analysis = {}
        if 'baseline' in phase_stats and 'totality' in phase_stats:
            baseline_coh = phase_stats['baseline']['mean_coherence']
            totality_coh = phase_stats['totality']['mean_coherence']
            
            if baseline_coh != 0:
                enhancement_ratio = totality_coh / baseline_coh
                enhancement_analysis = {
                    'baseline_coherence': baseline_coh,
                    'totality_coherence': totality_coh,
                    'enhancement_ratio': enhancement_ratio,
                    'enhancement_percent': (enhancement_ratio - 1) * 100,
                    'significant': bool(abs(enhancement_ratio - 1) > 0.1)  # 10% threshold
                }
        
        # Overall interpretation
        if enhancement_analysis.get('significant', False):
            ratio = enhancement_analysis['enhancement_ratio']
            percent = enhancement_analysis['enhancement_percent']
            direction = "enhancement" if ratio > 1 else "suppression"
            interpretation = f"Significant eclipse {direction}: {abs(percent):.1f}% change in timing coherence"
        else:
            interpretation = "No significant eclipse signature detected at 5-minute resolution"
        
        return {
            'success': True,
            'analysis_type': 'high_resolution_solar_eclipse',
            'eclipse_datetime': eclipse_date.isoformat(),
            'temporal_resolution_minutes': 5,
            'analysis_center': analysis_center,
            'temporal_evolution': temporal_evolution,
            'phase_statistics': phase_stats,
            'enhancement_analysis': enhancement_analysis,
            'interpretation': interpretation,
            'n_temporal_bins': len(temporal_evolution),
            'total_measurements': len(combined_df),
            'eclipse_window_pairs': len(coherence_df)
        }
        
    except Exception as e:
        print_status(f"High-resolution eclipse analysis failed: {e}", "ERROR")
        return {'success': False, 'error': str(e)}

def calculate_eclipse_parameters(lat: float, lon: float, eclipse_datetime: datetime) -> Dict:
    """
    Calculate precise eclipse parameters for a given location and time.
    
    This is a simplified eclipse calculation. For production use, consider
    using a proper astronomical library like pyephem or astropy.
    """
    try:
        # April 8, 2024 Total Solar Eclipse parameters
        # Eclipse path: Mexico -> USA -> Canada
        # Maximum eclipse: 18:18 UTC over North America
        
        # Simplified eclipse path calculation
        # Eclipse center line approximation (very simplified)
        eclipse_center_lat = 40.0  # Approximate latitude of eclipse center
        eclipse_center_lon = -85.0  # Approximate longitude at maximum
        
        # Calculate distance from eclipse center line
        lat_diff = lat - eclipse_center_lat
        lon_diff = lon - eclipse_center_lon
        distance_from_center = math.sqrt(lat_diff**2 + (lon_diff * math.cos(math.radians(lat)))**2) * 111.0  # km per degree
        
        # Eclipse timing calculation (simplified)
        # Eclipse moves west to east at approximately 2000 km/h
        eclipse_speed_kmh = 2000.0
        
        # Calculate local eclipse timing based on longitude
        # More western locations see eclipse earlier
        time_offset_hours = (lon - eclipse_center_lon) / 15.0  # 15 degrees per hour
        local_eclipse_time = eclipse_datetime + timedelta(hours=time_offset_hours)
        
        # Calculate eclipse magnitude based on distance from center
        if distance_from_center < 100:  # Totality zone
            eclipse_magnitude = 1.0
            eclipse_type = 'total'
        elif distance_from_center < 300:  # Deep partial
            eclipse_magnitude = 0.9 - (distance_from_center - 100) / 200 * 0.3
            eclipse_type = 'partial_deep'
        elif distance_from_center < 500:  # Moderate partial
            eclipse_magnitude = 0.6 - (distance_from_center - 300) / 200 * 0.3
            eclipse_type = 'partial_moderate'
        elif distance_from_center < 1000:  # Light partial
            eclipse_magnitude = 0.3 - (distance_from_center - 500) / 500 * 0.3
            eclipse_type = 'partial_light'
        else:  # No eclipse
            eclipse_magnitude = 0.0
            eclipse_type = 'none'
        
        # Calculate eclipse duration (simplified)
        if eclipse_magnitude > 0.8:
            duration_minutes = 4.0 - (distance_from_center / 100) * 2.0
        elif eclipse_magnitude > 0.5:
            duration_minutes = 2.0 - (distance_from_center / 300) * 1.0
        else:
            duration_minutes = 1.0
        
        duration_minutes = max(0.5, duration_minutes)
        
        # Calculate eclipse start and end times
        eclipse_start = local_eclipse_time - timedelta(minutes=duration_minutes/2)
        eclipse_end = local_eclipse_time + timedelta(minutes=duration_minutes/2)
        
        return {
            'success': True,
            'local_eclipse_time': local_eclipse_time,
            'eclipse_start': eclipse_start,
            'eclipse_end': eclipse_end,
            'eclipse_magnitude': eclipse_magnitude,
            'eclipse_type': eclipse_type,
            'duration_minutes': duration_minutes,
            'distance_from_center_km': distance_from_center,
            'eclipse_speed_kmh': eclipse_speed_kmh
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_eclipse_differential_coherence(analysis_center: str = 'merged', resolution: str = '1min', eclipse_date: str = '2024-04-08') -> Dict:
    """
    Performs a Dynamic Differential Coherence Analysis of the eclipse.
    This is the most precise method, accounting for the real-time movement of the eclipse shadow.
    
    Args:
        analysis_center: GPS analysis center ('code', 'igs_combined', 'esa_final', 'merged')
        resolution: Temporal resolution ('30s', '1min', '5min', etc.)
        eclipse_date: Eclipse date in YYYY-MM-DD format (default: '2024-04-08')
    """
    try:
        print_status("Starting Dynamic Differential Coherence Analysis...", "PROCESS")
        print_status("Using time-dependent eclipse shadow model for precise pair categorization.", "INFO")

        # --- Load Station and Eclipse Path Data ---
        station_coords = pd.read_csv(ROOT / "data/coordinates/station_coords_global.csv")
        station_coords['short_code'] = station_coords['code'].str[:4]
        station_coords = station_coords.drop_duplicates(subset=['short_code']).set_index('short_code')
        
        # Parse eclipse date and configure eclipse path
        eclipse_configs = {
            '2023-04-20': {  # Hybrid Solar Eclipse - Indian Ocean/Australia
                'start_utc': datetime(2023, 4, 20, 1, 37), 'start_lat': -9.6, 'start_lon': 125.7,
                'max_utc': datetime(2023, 4, 20, 4, 17), 'max_lat': -10.4, 'max_lon': 129.9,
                'end_utc': datetime(2023, 4, 20, 6, 56), 'end_lat': -11.2, 'end_lon': 134.1,
                'type': 'hybrid'
            },
            '2023-10-14': {  # Annular Solar Eclipse - Americas
                'start_utc': datetime(2023, 10, 14, 17, 4), 'start_lat': 42.0, 'start_lon': -125.0,
                'max_utc': datetime(2023, 10, 14, 18, 0), 'max_lat': 32.0, 'max_lon': -109.0,
                'end_utc': datetime(2023, 10, 14, 20, 55), 'end_lat': 15.0, 'end_lon': -82.0,
                'type': 'annular'
            },
            '2024-04-08': {  # Total Solar Eclipse - North America
                'start_utc': datetime(2024, 4, 8, 16, 38), 'start_lat': 25.0, 'start_lon': -105.0,
                'max_utc': datetime(2024, 4, 8, 18, 18), 'max_lat': 40.0, 'max_lon': -85.0,
                'end_utc': datetime(2024, 4, 8, 19, 55), 'end_lat': 47.0, 'end_lon': -53.0,
                'type': 'total'
            },
            '2024-10-02': {  # Annular Solar Eclipse - South Pacific/South America
                'start_utc': datetime(2024, 10, 2, 18, 42), 'start_lat': -22.4, 'start_lon': -175.2,
                'max_utc': datetime(2024, 10, 2, 20, 45), 'max_lat': -27.0, 'max_lon': -70.0,
                'end_utc': datetime(2024, 10, 2, 22, 47), 'end_lat': -31.6, 'end_lon': -64.8,
                'type': 'annular'
            },
            '2025-03-29': {  # Partial Solar Eclipse - Europe/Asia/Africa/North America
                'start_utc': datetime(2025, 3, 29, 8, 47), 'start_lat': 60.0, 'start_lon': -30.0,
                'max_utc': datetime(2025, 3, 29, 10, 48), 'max_lat': 65.0, 'max_lon': 0.0,
                'end_utc': datetime(2025, 3, 29, 12, 49), 'end_lat': 70.0, 'end_lon': 30.0,
                'type': 'partial'
            }
        }
        
        if eclipse_date not in eclipse_configs:
            return {
                'success': False,
                'error': f'Eclipse date {eclipse_date} not configured. Available: {list(eclipse_configs.keys())}'
            }
        
        eclipse_path = eclipse_configs[eclipse_date]
        total_duration = (eclipse_path['end_utc'] - eclipse_path['start_utc']).total_seconds()

        def get_shadow_center(current_time: datetime) -> Tuple[float, float]:
            """Interpolates the eclipse shadow's center coordinates for a given time."""
            if current_time <= eclipse_path['start_utc']:
                return eclipse_path['start_lat'], eclipse_path['start_lon']
            if current_time >= eclipse_path['end_utc']:
                return eclipse_path['end_lat'], eclipse_path['end_lon']
            
            fraction = (current_time - eclipse_path['start_utc']).total_seconds() / total_duration
            lat = eclipse_path['start_lat'] + fraction * (eclipse_path['end_lat'] - eclipse_path['start_lat'])
            lon = eclipse_path['start_lon'] + fraction * (eclipse_path['end_lon'] - eclipse_path['start_lon'])
            return lat, lon

        def get_dynamic_magnitude(station_lat, station_lon, current_time):
            shadow_lat, shadow_lon = get_shadow_center(current_time)
            dist_km = haversine_distance(station_lat, station_lon, shadow_lat, shadow_lon)
            # Magnitude decreases linearly up to ~2500 km
            magnitude = 1.0 - (dist_km / 2500.0)
            return max(0, min(1, magnitude))

        # --- Data Loading (from temporal analysis) ---
        all_clock_data = []
        centers_to_load = ['igs_combined', 'code', 'esa_final'] if analysis_center == 'merged' else [analysis_center]
        
        for center in centers_to_load:
            print_status(f"Loading data for {center.upper()}...", "PROCESS")
            data_dir = ROOT / "data" / "raw" / center
            prefixes = {
                'code': 'COD0OPSFIN_',
                'igs_combined': 'IGS0OPSFIN_',
                'esa_final': 'ESA0OPSFIN_'
            }
            
            if center not in prefixes:
                print_status(f"Unknown analysis center: {center}", "WARNING")
                continue
            
            prefix = prefixes[center]
            
            eclipse_files = get_eclipse_files(eclipse_date, data_dir, prefix)
            
            for clk_file in eclipse_files:
                if clk_file.exists():
                    df = parse_clk_file_high_resolution(clk_file)
                    if not df.empty:
                        all_clock_data.append(df)
        
        if not all_clock_data:
            return {'success': False, 'error': 'No CLK data found for the specified center(s)'}
        
        combined_df = pd.concat(all_clock_data, ignore_index=True)
        
        # If merged, handle duplicates by averaging
        if analysis_center == 'merged' and not combined_df.empty:
            print_status(f"Merging data from {len(centers_to_load)} centers. Original measurements: {len(combined_df):,}", "INFO")
            # Ensure station codes are standardized to 4 characters for grouping
            combined_df['station_group'] = combined_df['station'].str[:4]
            combined_df = combined_df.groupby(['datetime', 'station_group']).agg({
                'clock_bias': 'mean',
                'clock_drift': 'mean'
            }).reset_index().rename(columns={'station_group': 'station'})
            print_status(f"Merged and deduplicated measurements: {len(combined_df):,}", "SUCCESS")
        
        print_status(f"Loaded a total of {len(combined_df):,} clock measurements", "SUCCESS")
        
        # Filter to eclipse progression period
        eclipse_data = combined_df[
            (combined_df['datetime'] >= eclipse_path['start_utc']) &
            (combined_df['datetime'] <= eclipse_path['end_utc'])
        ].copy()
        
        if eclipse_data.empty:
            return {'success': False, 'error': 'No data during eclipse progression'}
        
        print_status(f"Eclipse period data: {len(eclipse_data):,} measurements", "SUCCESS")
        
        # Create time bins based on requested resolution for eclipse progression tracking
        resolution_map = {'30s': '30s', '1min': '1min', '5min': '5min', '15min': '15min', '30min': '30min'}
        time_resolution = resolution_map.get(resolution, '5min')
        eclipse_data['time_bin'] = eclipse_data['datetime'].dt.floor(time_resolution)
        
        print_status(f"Using {time_resolution} temporal resolution for eclipse tracking", "INFO")
        
        # --- Differential Coherence Analysis ---
        print_status("Starting Differential Coherence Analysis...", "PROCESS")
        
        # Track coherence evolution across eclipse progression
        temporal_evolution = []
        
        for time_bin, time_data in eclipse_data.groupby('time_bin'):
            
            # --- Pre-filter the data for this bin ---
            known_station_codes = station_coords.index
            time_data = time_data[time_data['station'].str[:4].isin(known_station_codes)]

            if time_data['station'].nunique() < 10:
                continue

            station_means = time_data.groupby('station')['clock_bias'].mean()
            known_stations_list = station_means.index.tolist()
            current_time = time_bin

            # --- Dynamic Magnitude Calculation ---
            station_magnitudes = {
                s: get_dynamic_magnitude(
                    station_coords.loc[s[:4], 'lat_deg'],
                    station_coords.loc[s[:4], 'lon_deg'],
                    current_time
                ) for s in known_stations_list
            }

            # --- Pair Categorization ---
            pair_coherences = {
                'intra_eclipse': [], # Both stations in an eclipse zone
                'intra_distant': [], # Both stations distant (control)
                'inter_gradient': [] # One in eclipse, one distant
            }
            
            for i, s1 in enumerate(known_stations_list):
                for j, s2 in enumerate(known_stations_list):
                    if i < j:
                        mag1 = station_magnitudes.get(s1, 0)
                        mag2 = station_magnitudes.get(s2, 0)

                        is_s1_eclipse = mag1 > 0.1
                        is_s2_eclipse = mag2 > 0.1

                        # Calculate coherence
                        bias1 = station_means[s1]
                        bias2 = station_means[s2]
                        coherence = 1.0 / (1.0 + (abs(bias1 - bias2) / 1e-4))

                        # Categorize the pair
                        if is_s1_eclipse and is_s2_eclipse:
                            pair_coherences['intra_eclipse'].append(coherence)
                        elif not is_s1_eclipse and not is_s2_eclipse:
                            pair_coherences['intra_distant'].append(coherence)
                        elif is_s1_eclipse != is_s2_eclipse:
                            pair_coherences['inter_gradient'].append(coherence)
            
            # --- Aggregate and Store Results ---
            pair_stats = {}
            for category, coherences in pair_coherences.items():
                if len(coherences) > 1: # Require at least 2 pairs for stats
                    pair_stats[category] = {
                        'mean_coherence': float(np.mean(coherences)),
                        'std_coherence': float(np.std(coherences)),
                        'n_pairs': len(coherences)
                    }

            if 'inter_gradient' not in pair_stats: # Skip if no gradient pairs
                continue

            temporal_evolution.append({
                'datetime': time_bin.isoformat(),
                'current_time': current_time.isoformat(), # Store the actual current_time
                'eclipse_params': calculate_eclipse_parameters(
                    get_shadow_center(current_time)[0], # Use interpolated latitude
                    get_shadow_center(current_time)[1], # Use interpolated longitude
                    current_time
                ), # Store dynamic eclipse parameters
                'pair_coherence_stats': pair_stats,
                'n_stations': len(known_stations_list),
                'eclipse_phase': get_eclipse_phase(
                    (current_time - pd.Timestamp(eclipse_path['max_utc'])).total_seconds() / 60
                )
            })

        if not temporal_evolution:
            return {'success': False, 'error': 'No temporal evolution data computed with differential analysis'}
        
        print_status(f"Tracked differential coherence across {len(temporal_evolution)} time periods", "SUCCESS")

        # --- Analyze Differential Signature ---
        differential_signature = {}
        phases = ['pre_eclipse_baseline', 'pre_eclipse', 'partial_beginning', 'maximum_totality', 'partial_ending', 'post_eclipse', 'post_eclipse_recovery']
        
        for category in ['intra_eclipse', 'intra_distant', 'inter_gradient']:
            category_evolution = []
            for phase in phases:
                # Correctly filter temporal_evolution based on the calculated eclipse_phase string
                phase_data = [t for t in temporal_evolution if t['eclipse_phase'] == phase and category in t.get('pair_coherence_stats', {})]
                if phase_data:
                    mean_coh = np.mean([t['pair_coherence_stats'][category]['mean_coherence'] for t in phase_data])
                    n_pairs_avg = np.mean([t['pair_coherence_stats'][category]['n_pairs'] for t in phase_data])
                    category_evolution.append({
                        'phase': phase,
                        'mean_coherence': float(mean_coh),
                        'avg_n_pairs': int(n_pairs_avg)
                    })
            if category_evolution:
                differential_signature[category] = category_evolution

        # --- Interpretation ---
        interpretation = "Differential Coherence Analysis complete. "
        try:
            baseline_gradient = next(p['mean_coherence'] for p in differential_signature['inter_gradient'] if p['phase'] == 'pre_eclipse_baseline')
            eclipse_gradient = next(p['mean_coherence'] for p in differential_signature['inter_gradient'] if p['phase'] == 'maximum_totality')
            change = (eclipse_gradient / baseline_gradient - 1) * 100
            interpretation += f"Gradient pairs show a {change:+.2f}% change during eclipse maximum."
        except (KeyError, StopIteration):
            interpretation += "Could not determine gradient change at maximum."

        # Convert datetime objects to strings for JSON serialization
        for t in temporal_evolution:
            if 'eclipse_params' in t and 'local_eclipse_time' in t['eclipse_params']:
                t['eclipse_params']['local_eclipse_time'] = t['eclipse_params']['local_eclipse_time'].isoformat()
            if 'eclipse_params' in t and 'eclipse_start' in t['eclipse_params']:
                t['eclipse_params']['eclipse_start'] = t['eclipse_params']['eclipse_start'].isoformat()
            if 'eclipse_params' in t and 'eclipse_end' in t['eclipse_params']:
                t['eclipse_params']['eclipse_end'] = t['eclipse_params']['eclipse_end'].isoformat()

        return {
            'success': True,
            'analysis_type': 'eclipse_differential_coherence',
            'analysis_center': analysis_center,
            'eclipse_date': '2024-04-08',
            'temporal_resolution_minutes': 0.5 if time_resolution == '30s' else int(time_resolution.replace('min', '')),
            'differential_signature': differential_signature,
            'temporal_evolution': temporal_evolution,
            'interpretation': interpretation
        }
        
    except Exception as e:
        print_status(f"Eclipse temporal-spatial analysis failed: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

def get_eclipse_phase(minutes_from_max: float) -> str:
    """Helper function to determine eclipse phase from minutes."""
    if minutes_from_max < -240: return 'pre_eclipse_baseline'
    if minutes_from_max < -120: return 'pre_eclipse'
    if minutes_from_max < -30: return 'partial_beginning'
    if minutes_from_max <= 30: return 'maximum_totality'
    if minutes_from_max <= 120: return 'partial_ending'
    if minutes_from_max <= 240: return 'post_eclipse'
    return 'post_eclipse_recovery'

def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Calculate the great-circle distance between two points on the Earth's surface.
    This is a simplified version for demonstration. For precise calculations,
    consider using a library like geopy or shapely.
    """
    R = 6371.0 # Radius of Earth in km
    d_lat = math.radians(lat2 - lat1)
    d_lon = math.radians(lon2 - lon1)
    a = math.sin(d_lat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(d_lon/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    distance = R * c
    return distance

def get_eclipse_files(eclipse_date: str, data_dir: Path, prefix: str) -> list:
    """Get the appropriate CLK files for a given eclipse date."""
    eclipse_file_configs = {
        '2023-04-20': {  # Day 110 of 2023
            'day_before': f"{prefix}20231090000_01D_30S_CLK.CLK.gz",
            'eclipse_day': f"{prefix}20231100000_01D_30S_CLK.CLK.gz",
            'day_after': f"{prefix}20231110000_01D_30S_CLK.CLK.gz"
        },
        '2023-10-14': {  # Day 287 of 2023
            'day_before': f"{prefix}20232860000_01D_30S_CLK.CLK.gz",
            'eclipse_day': f"{prefix}20232870000_01D_30S_CLK.CLK.gz",
            'day_after': f"{prefix}20232880000_01D_30S_CLK.CLK.gz"
        },
        '2024-04-08': {  # Day 099 of 2024
            'day_before': f"{prefix}20240980000_01D_30S_CLK.CLK.gz",
            'eclipse_day': f"{prefix}20240990000_01D_30S_CLK.CLK.gz", 
            'day_after': f"{prefix}20241000000_01D_30S_CLK.CLK.gz"
        },
        '2024-10-02': {  # Day 276 of 2024
            'day_before': f"{prefix}20242750000_01D_30S_CLK.CLK.gz",
            'eclipse_day': f"{prefix}20242760000_01D_30S_CLK.CLK.gz",
            'day_after': f"{prefix}20242770000_01D_30S_CLK.CLK.gz"
        },
        '2025-03-29': {  # Day 088 of 2025
            'day_before': f"{prefix}20250870000_01D_30S_CLK.CLK.gz",
            'eclipse_day': f"{prefix}20250880000_01D_30S_CLK.CLK.gz",
            'day_after': f"{prefix}20250890000_01D_30S_CLK.CLK.gz"
        }
    }
    
    if eclipse_date not in eclipse_file_configs:
        return []
    
    config = eclipse_file_configs[eclipse_date]
    return [
        data_dir / config['day_before'],
        data_dir / config['eclipse_day'],
        data_dir / config['day_after']
    ]

def load_geospatial_data(analysis_center: str) -> pd.DataFrame:
    """
    Load geospatial correlation data for the specified analysis center.
    Handles 'merged' center by combining all three centers.
    """
    data_dir = ROOT / "data/processed"
    
    if analysis_center == 'merged':
        # Load and merge all three centers
        center_files = [
            data_dir / "step_4_geospatial_code.csv",
            data_dir / "step_4_geospatial_igs_combined.csv", 
            data_dir / "step_4_geospatial_esa_final.csv"
        ]
        
        dfs = []
        for file_path in center_files:
            if file_path.exists():
                center_df = pd.read_csv(file_path)
                dfs.append(center_df)
        
        if not dfs:
            raise FileNotFoundError('No geospatial files found for any center')
        
        # Combine and average overlapping measurements
        df = pd.concat(dfs, ignore_index=True)
        df['date'] = pd.to_datetime(df['date'])
        
        # Calculate coherence from plateau_phase
        df['coherence'] = np.cos(df['plateau_phase'])
        
        # Average coherence for same date-station pairs
        df = df.groupby(['date', 'station_i', 'station_j'], as_index=False).agg({
            'coherence': 'mean',
            'dist_km': 'first'  # Distance should be the same
        })
        
        # Rename columns for consistency
        df = df.rename(columns={'station_i': 'station1', 'station_j': 'station2', 'dist_km': 'distance_km'})
    else:
        geospatial_file = data_dir / f"step_4_geospatial_{analysis_center}.csv"
        
        if not geospatial_file.exists():
            raise FileNotFoundError(f'Geospatial file not found: {geospatial_file}')
        
        df = pd.read_csv(geospatial_file)
        df['date'] = pd.to_datetime(df['date'])
        
        # Calculate coherence from plateau_phase and rename columns for consistency
        df['coherence'] = np.cos(df['plateau_phase'])
        df = df.rename(columns={'station_i': 'station1', 'station_j': 'station2', 'dist_km': 'distance_km'})
    
    return df

def analyze_jupiter_opposition_high_resolution_DEPRECATED(analysis_center: str = 'merged') -> Dict:
    """
    Analyze Jupiter opposition effects using proper orbital mechanics approach.
    
    CORRECTED METHOD: Uses continuous gravitational field modeling over 
    Â±120 day windows instead of flawed Â±5 day arbitrary windows.
    
    Expected correlation: ~0.08% (realistic based on orbital mechanics)
    Key dates: Nov 3, 2023 and Dec 7, 2024
    
    This replaces the previous method that produced artifacts of 17-81%.
    """
    try:
        print_status("Starting Jupiter Opposition High-Resolution Analysis...", "PROCESS")
        print_status("Analyzing gravitational potential coupling during Jupiter oppositions", "INFO")
        
        # Jupiter opposition dates
        jupiter_dates = ['2023-11-03', '2024-12-07']
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        results = {'success': True, 'analysis_center': analysis_center, 'jupiter_oppositions': []}
        
        for jupiter_date in jupiter_dates:
            event_date = pd.to_datetime(jupiter_date)
            
            # Extended window: Â±120 days for proper orbital mechanics analysis
            window_start = event_date - pd.Timedelta(days=120)
            window_end = event_date + pd.Timedelta(days=120)
            
            # Group data by day for efficiency and robustness
            daily_data = df.groupby(df['date'].dt.date).agg({
                'coherence': ['median', 'std', 'count']
            }).reset_index()
            daily_data.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
            daily_data['date'] = pd.to_datetime(daily_data['date'])
            
            # Filter to analysis window
            window_data = daily_data[
                (daily_data['date'] >= window_start) & 
                (daily_data['date'] <= window_end)
            ].copy()
            
            if len(window_data) < 50:  # Need sufficient data for orbital correlation
                continue
            
            # Calculate orbital mechanics parameters (inline calculation)
            try:
                # Simple orbital mechanics calculation for Jupiter
                # Jupiter orbital parameters: period = 11.86 years, average distance = 5.20 AU
                jupiter_period_days = 11.86 * 365.25
                jupiter_avg_distance_au = 5.20
                
                # Calculate days from opposition for each date
                window_data['days_from_opposition'] = (window_data['date'] - event_date).dt.days
                
                # Calculate Jupiter distance from Earth using simplified orbital mechanics
                # At opposition: distance â 5.20 - 1.0 = 4.20 AU (minimum)
                # At conjunction: distance â 5.20 + 1.0 = 6.20 AU (maximum)
                # Use cosine approximation for orbital position
                orbital_phase = (window_data['days_from_opposition'] / jupiter_period_days) * 2 * np.pi
                window_data['distance_au'] = 5.20 + 1.0 * np.cos(orbital_phase)
                
                # Calculate gravitational potential (proportional to 1/distance)
                window_data['gravitational_potential'] = 1.0 / (window_data['distance_au'] ** 2)
                
                # Calculate correlation between coherence and gravitational potential
                correlation = window_data['coherence_median'].corr(window_data['gravitational_potential'])
                
                # Realistic effect size: correlation * 100 (much smaller than previous artifacts)
                orbital_effect_percent = float(correlation * 100) if not np.isnan(correlation) else 0.0
                
                opposition_result = {
                    'date': jupiter_date,
                    'correlation_with_potential': float(correlation) if not np.isnan(correlation) else 0.0,
                    'orbital_effect_percent': orbital_effect_percent,
                    'minimum_distance_au': float(window_data['distance_au'].min()),
                    'maximum_distance_au': float(window_data['distance_au'].max()),
                    'data_points': len(window_data),
                    'window_days': 240,
                    'method': 'orbital_mechanics_inline'
                }
                
                results['jupiter_oppositions'].append(opposition_result)
                print_status(f"Jupiter opposition {jupiter_date}: {orbital_effect_percent:+.4f}% orbital correlation", "SUCCESS")
                    
            except Exception as calc_error:
                print_status(f"Orbital calculation failed for {jupiter_date}: {calc_error}", "WARNING")
                continue
        
        # Calculate average effect using proper orbital mechanics
        if results['jupiter_oppositions']:
            avg_correlation = np.mean([r.get('correlation_with_potential', 0) for r in results['jupiter_oppositions']])
            results['average_correlation'] = float(avg_correlation)
            results['interpretation'] = f"Jupiter oppositions show orbital mechanics correlation: {avg_correlation:+.4f}"
        else:
            results['interpretation'] = "Insufficient data for Jupiter opposition analysis"
            
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_saturn_opposition_high_resolution_DEPRECATED(analysis_center: str = 'merged') -> Dict:
    """
    Analyze Saturn opposition effects using proper orbital mechanics approach.
    
    CORRECTED METHOD: Uses continuous gravitational field modeling over 
    Â±120 day windows instead of flawed Â±5 day arbitrary windows.
    
    Saturn oppositions: Aug 27, 2023; Sep 8, 2024; Sep 21, 2025
    Expected correlation: ~0.02% (realistic based on orbital mechanics)
    
    This replaces the previous method that produced artifacts.
    """
    try:
        print_status("Starting Saturn Opposition High-Resolution Analysis...", "PROCESS")
        
        saturn_dates = ['2023-08-27', '2024-09-08', '2025-09-21']
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        results = {'success': True, 'analysis_center': analysis_center, 'saturn_oppositions': []}
        
        for saturn_date in saturn_dates:
            event_date = pd.to_datetime(saturn_date)
            
            # Extended window: Â±120 days for proper orbital mechanics analysis
            window_start = event_date - pd.Timedelta(days=120)
            window_end = event_date + pd.Timedelta(days=120)
            
            # Group data by day for efficiency and robustness
            daily_data = df.groupby(df['date'].dt.date).agg({
                'coherence': ['median', 'std', 'count']
            }).reset_index()
            daily_data.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
            daily_data['date'] = pd.to_datetime(daily_data['date'])
            
            # Filter to analysis window
            window_data = daily_data[
                (daily_data['date'] >= window_start) & 
                (daily_data['date'] <= window_end)
            ].copy()
            
            if len(window_data) < 50:  # Need sufficient data for orbital correlation
                continue
            
            # Calculate orbital mechanics parameters (inline calculation)
            try:
                # Simple orbital mechanics calculation for Saturn
                # Saturn orbital parameters: period = 29.46 years, average distance = 9.54 AU
                saturn_period_days = 29.46 * 365.25
                saturn_avg_distance_au = 9.54
                
                # Calculate days from opposition for each date
                window_data['days_from_opposition'] = (window_data['date'] - event_date).dt.days
                
                # Calculate Saturn distance from Earth using simplified orbital mechanics
                # At opposition: distance â 9.54 - 1.0 = 8.54 AU (minimum)
                # At conjunction: distance â 9.54 + 1.0 = 10.54 AU (maximum)
                # Use cosine approximation for orbital position
                orbital_phase = (window_data['days_from_opposition'] / saturn_period_days) * 2 * np.pi
                window_data['distance_au'] = 9.54 + 1.0 * np.cos(orbital_phase)
                
                # Calculate gravitational potential (proportional to 1/distance)
                window_data['gravitational_potential'] = 1.0 / (window_data['distance_au'] ** 2)
                
                # Calculate correlation between coherence and gravitational potential
                correlation = window_data['coherence_median'].corr(window_data['gravitational_potential'])
                
                # Realistic effect size: correlation * 100 (much smaller than previous artifacts)
                orbital_effect_percent = float(correlation * 100) if not np.isnan(correlation) else 0.0
                
                opposition_result = {
                    'date': saturn_date,
                    'correlation_with_potential': float(correlation) if not np.isnan(correlation) else 0.0,
                    'orbital_effect_percent': orbital_effect_percent,
                    'minimum_distance_au': float(window_data['distance_au'].min()),
                    'maximum_distance_au': float(window_data['distance_au'].max()),
                    'data_points': len(window_data),
                    'window_days': 240,
                    'method': 'orbital_mechanics_inline'
                }
                
                results['saturn_oppositions'].append(opposition_result)
                print_status(f"Saturn opposition {saturn_date}: {orbital_effect_percent:+.4f}% orbital correlation", "SUCCESS")
                    
            except Exception as calc_error:
                print_status(f"Orbital calculation failed for {saturn_date}: {calc_error}", "WARNING")
                continue
        
        if results['saturn_oppositions']:
            avg_correlation = np.mean([r.get('correlation_with_potential', 0) for r in results['saturn_oppositions']])
            results['average_correlation'] = float(avg_correlation)
            results['interpretation'] = f"Saturn oppositions show orbital mechanics correlation: {avg_correlation:+.4f}"
        else:
            results['interpretation'] = "Insufficient data for Saturn opposition analysis"
            
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_mars_opposition_high_resolution_DEPRECATED(analysis_center: str = 'merged') -> Dict:
    """
    Analyze Mars opposition effects using proper orbital mechanics approach.
    
    CORRECTED METHOD: Uses continuous gravitational field modeling over 
    Â±120 day windows instead of flawed Â±5 day arbitrary windows.
    
    Mars opposition: Jan 16, 2025
    Expected correlation: ~0.04% (realistic based on orbital mechanics)
    
    This replaces the previous method that produced artifacts.
    """
    try:
        print_status("Starting Mars Opposition High-Resolution Analysis...", "PROCESS")
        
        mars_dates = ['2025-01-16']
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        results = {'success': True, 'analysis_center': analysis_center, 'mars_oppositions': []}
        
        for mars_date in mars_dates:
            event_date = pd.to_datetime(mars_date)
            
            # Extended window: Â±120 days for proper orbital mechanics analysis
            window_start = event_date - pd.Timedelta(days=120)
            window_end = event_date + pd.Timedelta(days=120)
            
            # Group data by day for efficiency and robustness
            daily_data = df.groupby(df['date'].dt.date).agg({
                'coherence': ['median', 'std', 'count']
            }).reset_index()
            daily_data.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
            daily_data['date'] = pd.to_datetime(daily_data['date'])
            
            # Filter to analysis window
            window_data = daily_data[
                (daily_data['date'] >= window_start) & 
                (daily_data['date'] <= window_end)
            ].copy()
            
            if len(window_data) < 50:  # Need sufficient data for orbital correlation
                continue
            
            # Calculate orbital mechanics parameters (inline calculation)
            try:
                # Simple orbital mechanics calculation for Mars
                # Mars orbital parameters: period = 1.88 years, average distance = 1.52 AU
                mars_period_days = 1.88 * 365.25
                mars_avg_distance_au = 1.52
                
                # Calculate days from opposition for each date
                window_data['days_from_opposition'] = (window_data['date'] - event_date).dt.days
                
                # Calculate Mars distance from Earth using simplified orbital mechanics
                # At opposition: distance â 1.52 - 1.0 = 0.52 AU (minimum)
                # At conjunction: distance â 1.52 + 1.0 = 2.52 AU (maximum)
                # Use cosine approximation for orbital position
                orbital_phase = (window_data['days_from_opposition'] / mars_period_days) * 2 * np.pi
                window_data['distance_au'] = 1.52 + 1.0 * np.cos(orbital_phase)
                
                # Calculate gravitational potential (proportional to 1/distance)
                window_data['gravitational_potential'] = 1.0 / (window_data['distance_au'] ** 2)
                
                # Calculate correlation between coherence and gravitational potential
                correlation = window_data['coherence_median'].corr(window_data['gravitational_potential'])
                
                # Realistic effect size: correlation * 100 (much smaller than previous artifacts)
                orbital_effect_percent = float(correlation * 100) if not np.isnan(correlation) else 0.0
                
                opposition_result = {
                    'date': mars_date,
                    'correlation_with_potential': float(correlation) if not np.isnan(correlation) else 0.0,
                    'orbital_effect_percent': orbital_effect_percent,
                    'minimum_distance_au': float(window_data['distance_au'].min()),
                    'maximum_distance_au': float(window_data['distance_au'].max()),
                    'data_points': len(window_data),
                    'window_days': 240,
                    'method': 'orbital_mechanics_inline'
                }
                
                results['mars_oppositions'].append(opposition_result)
                print_status(f"Mars opposition {mars_date}: {orbital_effect_percent:+.4f}% orbital correlation", "SUCCESS")
                    
            except Exception as calc_error:
                print_status(f"Orbital calculation failed for {mars_date}: {calc_error}", "WARNING")
                continue
        
        if results['mars_oppositions']:
            avg_correlation = np.mean([r.get('correlation_with_potential', 0) for r in results['mars_oppositions']])
            results['average_correlation'] = float(avg_correlation)
            results['interpretation'] = f"Mars oppositions show orbital mechanics correlation: {avg_correlation:+.4f}"
        else:
            results['interpretation'] = "Insufficient data for Mars opposition analysis"
            
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_mercury_opposition_high_resolution_DEPRECATED(analysis_center: str = 'merged') -> Dict:
    """
    Analyze Mercury opposition effects using proper orbital mechanics approach.
    
    Mercury has the fastest orbital period (~88 days) and is closest to the Sun.
    Expected correlation: ~0.01-0.1% (small due to low mass but close distance)
    
    Key dates: Multiple oppositions per year due to fast orbit
    """
    try:
        print_status("Starting Mercury Opposition High-Resolution Analysis...", "PROCESS")
        print_status("Analyzing gravitational potential coupling during Mercury oppositions", "INFO")
        
        # Mercury opposition dates (approximate, multiple per year due to 88-day orbit)
        mercury_dates = [
            '2023-01-07', '2023-05-29', '2023-10-20', 
            '2024-03-11', '2024-08-01', '2024-12-22',
            '2025-05-13', '2025-10-03'
        ]
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        results = {'success': True, 'analysis_center': analysis_center, 'mercury_oppositions': []}
        
        for mercury_date in mercury_dates:
            event_date = pd.to_datetime(mercury_date)
            
            # Shorter window for Mercury due to fast orbit: Â±60 days
            window_start = event_date - pd.Timedelta(days=60)
            window_end = event_date + pd.Timedelta(days=60)
            
            # Group data by day for efficiency and robustness
            daily_data = df.groupby(df['date'].dt.date).agg({
                'coherence': ['median', 'std', 'count']
            }).reset_index()
            daily_data.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
            daily_data['date'] = pd.to_datetime(daily_data['date'])
            
            # Filter to analysis window
            window_data = daily_data[
                (daily_data['date'] >= window_start) & 
                (daily_data['date'] <= window_end)
            ].copy()
            
            if len(window_data) < 30:  # Need sufficient data for orbital correlation
                continue
            
            # Calculate orbital mechanics parameters (inline calculation)
            try:
                # Mercury orbital parameters: period = 88 days, average distance = 0.39 AU
                mercury_period_days = 88.0
                mercury_avg_distance_au = 0.39
                
                # Calculate days from opposition for each date
                window_data['days_from_opposition'] = (window_data['date'] - event_date).dt.days
                
                # Calculate Mercury distance from Earth using simplified orbital mechanics
                # At opposition: distance â 0.39 - 1.0 = -0.61 AU (impossible, so use minimum)
                # At conjunction: distance â 0.39 + 1.0 = 1.39 AU
                # Use cosine approximation for orbital position
                orbital_phase = (window_data['days_from_opposition'] / mercury_period_days) * 2 * np.pi
                window_data['distance_au'] = 0.39 + 1.0 * np.cos(orbital_phase)
                window_data['distance_au'] = np.maximum(window_data['distance_au'], 0.1)  # Minimum distance
                
                # Calculate gravitational potential (proportional to 1/distance^2)
                window_data['gravitational_potential'] = 1.0 / (window_data['distance_au'] ** 2)
                
                # Calculate correlation between coherence and gravitational potential
                correlation = window_data['coherence_median'].corr(window_data['gravitational_potential'])
                
                # Realistic effect size: correlation * 100 (small due to Mercury's low mass)
                orbital_effect_percent = float(correlation * 100) if not np.isnan(correlation) else 0.0
                
                opposition_result = {
                    'date': mercury_date,
                    'correlation_with_potential': float(correlation) if not np.isnan(correlation) else 0.0,
                    'orbital_effect_percent': orbital_effect_percent,
                    'minimum_distance_au': float(window_data['distance_au'].min()),
                    'maximum_distance_au': float(window_data['distance_au'].max()),
                    'data_points': len(window_data),
                    'window_days': 120,
                    'method': 'orbital_mechanics_inline'
                }
                
                results['mercury_oppositions'].append(opposition_result)
                print_status(f"Mercury opposition {mercury_date}: {orbital_effect_percent:+.4f}% orbital correlation", "SUCCESS")
                    
            except Exception as calc_error:
                print_status(f"Orbital calculation failed for {mercury_date}: {calc_error}", "WARNING")
                continue
        
        if results['mercury_oppositions']:
            avg_correlation = np.mean([r.get('correlation_with_potential', 0) for r in results['mercury_oppositions']])
            results['average_correlation'] = float(avg_correlation)
            results['interpretation'] = f"Mercury oppositions show orbital mechanics correlation: {avg_correlation:+.4f}"
        else:
            results['interpretation'] = "Insufficient data for Mercury opposition analysis"
            
        return results
        
    except Exception as e:
        print_status(f"Mercury opposition analysis failed: {e}", "ERROR")
        return {'success': False, 'error': str(e)}

def analyze_continuous_gravitational_field(analysis_center: str = 'merged') -> Dict:
    """
    Analyze continuous gravitational field effects using the complete 2.5-year dataset.
    
    This creates the "gravitational key" pattern by tracking Earth's orbital position
    and all planetary gravitational influences throughout the entire analysis period
    (2023-01-01 to 2025-06-30).
    
    Instead of discrete opposition events, this analyzes the continuous gravitational
    field pattern created by all planets simultaneously.
    """
    try:
        print_status("Starting Continuous Gravitational Field Analysis...", "PROCESS")
        print_status("Creating complete 'gravitational key' pattern for 2023-2025", "INFO")
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        # Define analysis period
        start_date = pd.to_datetime('2023-01-01')
        end_date = pd.to_datetime('2025-06-30')
        
        # Group data by day for efficiency
        daily_data = df.groupby(df['date'].dt.date).agg({
            'coherence': ['median', 'std', 'count']
        }).reset_index()
        daily_data.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
        daily_data['date'] = pd.to_datetime(daily_data['date'])
        
        # Filter to analysis period
        period_data = daily_data[
            (daily_data['date'] >= start_date) & 
            (daily_data['date'] <= end_date)
        ].copy()
        
        if len(period_data) < 100:
            return {'success': False, 'error': 'Insufficient data for continuous analysis'}
        
        print_status(f"Analyzing {len(period_data)} days of data from {start_date.date()} to {end_date.date()}", "INFO")
        
        # Calculate continuous gravitational potential for each day using proper orbital mechanics
        try:
            # Planetary masses (kg) - actual masses for proper gravitational calculations
            planet_masses_kg = {
                'mercury': 3.3011e23,    # kg
                'venus': 4.8675e24,      # kg  
                'mars': 6.4171e23,       # kg
                'jupiter': 1.8982e27,    # kg
                'saturn': 5.6834e26      # kg
            }
            
            # Gravitational constant (mÂ³/kg/sÂ²)
            G = 6.67430e-11
            
            # Orbital elements (approximate, for simplified calculations)
            # Semi-major axes (AU)
            semi_major_axes = {
                'mercury': 0.387,
                'venus': 0.723,
                'mars': 1.524,
                'jupiter': 5.203,
                'saturn': 9.537
            }
            
            # Orbital periods (days)
            orbital_periods = {
                'mercury': 87.97,
                'venus': 224.70,
                'mars': 686.98,
                'jupiter': 4332.59,
                'saturn': 10759.22
            }
            
            # Mean anomalies at epoch (degrees) - approximate starting positions
            mean_anomalies_epoch = {
                'mercury': 174.8,   # degrees at 2023-01-01
                'venus': 50.1,
                'mars': 19.4,
                'jupiter': 20.0,
                'saturn': 317.7
            }
            
            # Calculate days from start of analysis period
            period_data['days_from_start'] = (period_data['date'] - start_date).dt.days
            
            # Initialize total gravitational potential
            period_data['total_gravitational_potential'] = 0.0
            
            # Calculate gravitational potential from each planet using proper orbital mechanics
            for planet, mass_kg in planet_masses_kg.items():
                period = orbital_periods[planet]
                a = semi_major_axes[planet]  # AU
                M0 = np.radians(mean_anomalies_epoch[planet])  # Convert to radians
                
                # Calculate mean anomaly for each day
                # M = M0 + n * t, where n = 2Ï/P is mean motion
                mean_motion = 2 * np.pi / period  # radians per day
                mean_anomaly = M0 + mean_motion * period_data['days_from_start']
                
                # For simplified circular orbits, distance from Sun â semi-major axis
                # Distance from Earth = |r_planet - r_earth|
                # Assuming Earth at 1 AU and circular orbits
                earth_distance = 1.0  # AU
                
                # Calculate planet distance from Earth using law of cosines
                # d = sqrt(aÂ² + 1Â² - 2*a*cos(Î¸))
                # where Î¸ is the angle between Earth and planet as seen from Sun
                angle_difference = mean_anomaly - (2 * np.pi * period_data['days_from_start'] / 365.25)
                planet_distance_au = np.sqrt(a**2 + earth_distance**2 - 2*a*earth_distance*np.cos(angle_difference))
                
                # Convert AU to meters for gravitational calculations
                au_to_meters = 1.496e11  # meters per AU
                planet_distance_m = planet_distance_au * au_to_meters
                
                # Calculate gravitational potential: Î¦ = -GM/r (proper formula)
                # Use negative sign for gravitational potential
                planet_potential = -G * mass_kg / planet_distance_m
                
                # Add to total gravitational potential
                period_data['total_gravitational_potential'] += planet_potential
                
                # Store individual planet potentials for analysis
                period_data[f'{planet}_potential'] = planet_potential
                period_data[f'{planet}_distance_au'] = planet_distance_au
            
            # Calculate correlation between GPS coherence and total gravitational potential
            total_correlation = period_data['coherence_median'].corr(period_data['total_gravitational_potential'])
            
            # Calculate individual planet correlations
            planet_correlations = {}
            for planet in planet_masses_kg.keys():
                corr = period_data['coherence_median'].corr(period_data[f'{planet}_potential'])
                planet_correlations[planet] = float(corr) if not np.isnan(corr) else 0.0
            
            # Calculate effect sizes
            total_effect_percent = float(total_correlation * 100) if not np.isnan(total_correlation) else 0.0
            planet_effects = {planet: float(corr * 100) for planet, corr in planet_correlations.items()}
            
            # Statistical analysis
            potential_range = period_data['total_gravitational_potential'].max() - period_data['total_gravitational_potential'].min()
            coherence_range = period_data['coherence_median'].max() - period_data['coherence_median'].min()
            
            results = {
                'success': True,
                'analysis_center': analysis_center,
                'analysis_period': {
                    'start_date': start_date.strftime('%Y-%m-%d'),
                    'end_date': end_date.strftime('%Y-%m-%d'),
                    'total_days': len(period_data),
                    'data_points': int(period_data['coherence_count'].sum())
                },
                'gravitational_analysis': {
                    'total_correlation': float(total_correlation) if not np.isnan(total_correlation) else 0.0,
                    'total_effect_percent': total_effect_percent,
                    'planet_correlations': planet_correlations,
                    'planet_effects_percent': planet_effects,
                    'potential_range': float(potential_range),
                    'coherence_range': float(coherence_range)
                },
                'method': 'continuous_gravitational_field',
                'interpretation': f"Continuous gravitational field analysis shows {total_effect_percent:+.2f}% correlation with GPS coherence over {len(period_data)} days"
            }
            
            print_status(f"Continuous gravitational field correlation: {total_effect_percent:+.2f}%", "SUCCESS")
            print_status(f"Individual planet effects: {planet_effects}", "INFO")
            
            return results
            
        except Exception as calc_error:
            print_status(f"Gravitational field calculation failed: {calc_error}", "ERROR")
            return {'success': False, 'error': str(calc_error)}
        
    except Exception as e:
        print_status(f"Continuous gravitational field analysis failed: {e}", "ERROR")
        return {'success': False, 'error': str(e)}

def analyze_advanced_sham_controls(analysis_center: str = 'merged') -> Dict:
    """
    Advanced sham control testing to distinguish TEP effects from GPS processing artifacts.

    This implements sophisticated controls:
    1. Time-shuffled controls (preserve temporal structure, shuffle values)
    2. GPS processing-specific controls (known error patterns)
    3. Independent astronomical controls (non-gravitational effects)
    4. Multiple comparison corrections (FDR, Bonferroni)
    5. Permutation testing for robust significance
    """
    try:
        print_status("Starting Advanced Sham Control Analysis...", "PROCESS")
        print_status("Implementing sophisticated controls to distinguish TEP from artifacts", "INFO")

        # Import required libraries
        try:
            from jplephem.spk import SPK
            from scipy import stats
            from statsmodels.stats.multitest import multipletests
        except ImportError as e:
            return {'success': False, 'error': f'Required libraries not available: {e}'}

        # Load JPL ephemeris data
        try:
            kernel = SPK.open('de432s.bsp')
            print_status("JPL DE432s ephemeris loaded successfully", "SUCCESS")
        except Exception as e:
            return {'success': False, 'error': f'Failed to load JPL ephemeris: {e}'}

        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}

        # Define analysis period
        start_date = pd.to_datetime('2023-01-01')
        end_date = pd.to_datetime('2025-06-30')

        # Group data by day
        daily_data = df[
            (df['date'] >= start_date) &
            (df['date'] <= end_date)
        ].copy()

        # Group by date to get daily averages
        daily_grouped = daily_data.groupby(daily_data['date'].dt.date).agg({
            'coherence': ['median', 'std', 'count']
        }).reset_index()
        daily_grouped.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
        daily_grouped['date'] = pd.to_datetime(daily_grouped['date'])

        if len(daily_grouped) < 100:
            return {'success': False, 'error': 'Insufficient daily data for advanced testing'}

        print_status(f"Running advanced controls for {len(daily_grouped)} days", "INFO")

        # Planetary parameters
        planet_ids = {'mercury': 1, 'venus': 2, 'mars': 4, 'jupiter': 5, 'saturn': 6}
        planet_masses = {
            'mercury': 3.3011e23, 'venus': 4.8675e24, 'mars': 6.4171e23,
            'jupiter': 1.8982e27, 'saturn': 5.6834e26
        }
        G = 6.67430e-11

        # STEP 1: REAL GRAVITATIONAL FIELD ANALYSIS
        print_status("STEP 1: Real gravitational field analysis...", "INFO")
        real_sequence = _calculate_daily_field_sequence(
            daily_grouped, kernel, planet_ids, planet_masses, G, start_date
        )

        # STEP 2: ADVANCED SHAM CONTROLS
        print_status("STEP 2: Advanced sham controls...", "INFO")

        # 2A: Time-shuffled control (preserve temporal structure, shuffle coherence)
        print_status("  2A: Time-shuffled control...", "INFO")
        time_shuffled_sequence = _create_time_shuffled_control(
            daily_grouped, real_sequence, kernel, planet_ids, planet_masses, G, start_date
        )

        # 2B: GPS processing artifact controls
        print_status("  2B: GPS processing artifact controls...", "INFO")
        gps_controls = _create_gps_artifact_controls(daily_grouped)

        # 2C: Independent astronomical controls
        print_status("  2C: Independent astronomical controls...", "INFO")
        astronomical_controls = _create_astronomical_controls(daily_grouped)

        # STEP 3: CORRELATION ANALYSIS
        print_status("STEP 3: Comprehensive correlation analysis...", "INFO")

        results = {
            'success': True,
            'analysis_center': analysis_center,
            'analysis_period': {
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d'),
                'total_days': len(daily_grouped)
            },
            'real_correlations': {},
            'control_results': {},
            'statistical_validation': {}
        }

        # Calculate real correlations
        for planet in planet_ids.keys():
            real_corr = np.corrcoef(
                real_sequence['coherence_response'],
                real_sequence[f'{planet}_strength']
            )[0, 1]
            results['real_correlations'][planet] = float(real_corr) if not np.isnan(real_corr) else 0.0

        # Calculate control correlations
        all_correlations = []
        control_names = []

        # Time-shuffled control
        for planet in planet_ids.keys():
            shuffled_corr = np.corrcoef(
                time_shuffled_sequence['coherence_response'],
                time_shuffled_sequence[f'{planet}_strength']
            )[0, 1]
            results['control_results'][f'time_shuffled_{planet}'] = float(shuffled_corr) if not np.isnan(shuffled_corr) else 0.0
            all_correlations.append(shuffled_corr)
            control_names.append(f'time_shuffled_{planet}')

        # GPS artifact controls
        for control_name, control_data in gps_controls.items():
            if isinstance(control_data, dict) and 'correlation' in control_data:
                results['control_results'][control_name] = control_data['correlation']
                all_correlations.append(control_data['correlation'])
                control_names.append(control_name)

        # Astronomical controls
        for control_name, control_data in astronomical_controls.items():
            if isinstance(control_data, dict) and 'correlation' in control_data:
                results['control_results'][control_name] = control_data['correlation']
                all_correlations.append(control_data['correlation'])
                control_names.append(control_name)

        # STEP 4: STATISTICAL SIGNIFICANCE TESTING
        print_status("STEP 4: Statistical significance testing...", "INFO")

        # Permutation testing for robust significance
        n_permutations = 1000
        permutation_results = {}

        for planet in planet_ids.keys():
            real_corr = results['real_correlations'][planet]

            # Generate null distribution through permutations
            permuted_correlations = []
            for i in range(n_permutations):
                # Shuffle coherence values while keeping field strengths fixed
                shuffled_coherence = np.random.permutation(real_sequence['coherence_response'])
                permuted_corr = np.corrcoef(shuffled_coherence, real_sequence[f'{planet}_strength'])[0, 1]
                if not np.isnan(permuted_corr):
                    permuted_correlations.append(permuted_corr)

            if permuted_correlations:
                permuted_correlations = np.array(permuted_correlations)
                p_value = np.sum(permuted_correlations >= real_corr) / len(permuted_correlations)
                ci_lower = np.percentile(permuted_correlations, 2.5)
                ci_upper = np.percentile(permuted_correlations, 97.5)

                permutation_results[planet] = {
                    'p_value': float(p_value),
                    'significant': bool(p_value < 0.05),
                    'confidence_interval': [float(ci_lower), float(ci_upper)],
                    'interpretation': 'Permutation test with 1000 shuffles'
                }

        results['statistical_validation']['permutation_tests'] = permutation_results

        # Multiple comparison corrections
        print_status("STEP 5: Multiple comparison corrections...", "INFO")

        # Get p-values for all planets
        p_values = [permutation_results[planet]['p_value'] for planet in planet_ids.keys()]

        # Bonferroni correction
        bonferroni_results = multipletests(p_values, method='bonferroni')

        # FDR correction (Benjamini-Hochberg)
        fdr_results = multipletests(p_values, method='fdr_bh')

        results['statistical_validation']['multiple_comparison_corrections'] = {
            'bonferroni': {
                'corrected_p_values': [float(p) for p in bonferroni_results[1]],
                'significant': [bool(s.item()) if hasattr(s, 'item') else bool(s) for s in bonferroni_results[0]],
                'interpretation': 'Conservative correction for multiple testing'
            },
            'fdr_benjamini_hochberg': {
                'corrected_p_values': [float(p) for p in fdr_results[1]],
                'significant': [bool(s.item()) if hasattr(s, 'item') else bool(s) for s in fdr_results[0]],
                'interpretation': 'Less conservative correction for multiple testing'
            }
        }

        # STEP 6: ASSESSMENT AND INTERPRETATION
        print_status("STEP 6: Assessment and interpretation...", "INFO")

        # Calculate TEP effects beyond controls
        tep_effects_beyond_controls = {}

        for planet in planet_ids.keys():
            real_corr = results['real_correlations'][planet]

            # Compare with time-shuffled control
            shuffled_control = results['control_results'].get(f'time_shuffled_{planet}', 0.0)
            tep_effect = real_corr - shuffled_control

            tep_effects_beyond_controls[planet] = {
                'real_correlation': float(real_corr),
                'time_shuffled_control': float(shuffled_control),
                'tep_effect_beyond_control': float(tep_effect),
                'significant': bool(permutation_results[planet]['significant']),
                'corrected_significant_bonferroni': bool(results['statistical_validation']['multiple_comparison_corrections']['bonferroni']['significant'][list(planet_ids.keys()).index(planet)]),
                'corrected_significant_fdr': bool(results['statistical_validation']['multiple_comparison_corrections']['fdr_benjamini_hochberg']['significant'][list(planet_ids.keys()).index(planet)])
            }

        results['tep_effects_beyond_controls'] = tep_effects_beyond_controls

        # Final assessment
        print_status("ADVANCED SHAM CONTROL RESULTS:", "INFO")
        print_status("=" * 50, "INFO")

        planets = list(planet_ids.keys())
        for i, planet in enumerate(planets):
            tep_data = tep_effects_beyond_controls[planet]
            print_status(f"{planet.upper()}:", "INFO")
            print_status(f"  Real correlation: {tep_data['real_correlation']*100:+.2f}%", "INFO")
            print_status(f"  Time-shuffled control: {tep_data['time_shuffled_control']*100:+.2f}%", "INFO")
            print_status(f"  TEP effect beyond control: {tep_data['tep_effect_beyond_control']*100:+.2f}%", "INFO")
            print_status(f"  Significant (permutation): {tep_data['significant']}", "INFO")
            print_status(f"  Significant (Bonferroni): {tep_data['corrected_significant_bonferroni']}", "INFO")
            print_status(f"  Significant (FDR): {tep_data['corrected_significant_fdr']}", "INFO")

        return results

    except Exception as e:
        print_status(f"Advanced sham control analysis failed: {e}", "ERROR")
        return {'success': False, 'error': str(e)}

def _calculate_daily_field_sequence(daily_grouped, kernel, planet_ids, planet_masses, G, start_date):
    """Helper method to calculate daily gravitational field sequence."""
    sequence = {
        'coherence_response': [],
        'total_field_strength': []
    }

    for planet in planet_ids.keys():
        sequence[f'{planet}_strength'] = []

    for idx, row in daily_grouped.iterrows():
        date = row['date']
        jd = date.to_julian_date()

        try:
            earth_position = kernel[0, 3].compute(jd)
            total_field = 0.0

            for planet_name, planet_id in planet_ids.items():
                planet_position = kernel[0, planet_id].compute(jd)
                distance_km = np.sqrt(np.sum((planet_position - earth_position)**2))
                distance_m = distance_km * 1000

                field_strength = G * planet_masses[planet_name] / (distance_m ** 2)
                sequence[f'{planet_name}_strength'].append(field_strength)
                total_field += field_strength

            sequence['total_field_strength'].append(total_field)
            sequence['coherence_response'].append(row.get('coherence_median', 0.0))

        except Exception:
            # Fill with zeros for failed calculations
            for planet in planet_ids.keys():
                sequence[f'{planet}_strength'].append(0.0)
            sequence['total_field_strength'].append(0.0)
            sequence['coherence_response'].append(0.0)

    return sequence

def _create_time_shuffled_control(daily_grouped, real_sequence, kernel, planet_ids, planet_masses, G, start_date):
    """Create time-shuffled control (preserve temporal structure, shuffle coherence values)."""
    shuffled_sequence = {
        'coherence_response': np.random.permutation(real_sequence['coherence_response']),
        'total_field_strength': real_sequence['total_field_strength']
    }

    for planet in planet_ids.keys():
        shuffled_sequence[f'{planet}_strength'] = real_sequence[f'{planet}_strength']

    return shuffled_sequence

def _create_gps_artifact_controls(daily_grouped):
    """Create controls for known GPS processing artifacts."""
    controls = {}

    try:
        # Control 1: Seasonal patterns in GPS data
        days_from_start = (daily_grouped['date'] - daily_grouped['date'].min()).dt.days

        # Annual cycle (known GPS systematic effect)
        annual_pattern = np.sin(2 * np.pi * days_from_start / 365.25)
        annual_corr = daily_grouped['coherence_median'].corr(pd.Series(annual_pattern))
        controls['gps_annual_cycle'] = {
            'correlation': float(annual_corr) if not np.isnan(annual_corr) else 0.0,
            'interpretation': 'Known GPS seasonal systematic effect'
        }

        # Monthly cycle
        monthly_pattern = np.sin(2 * np.pi * days_from_start / 30.44)
        monthly_corr = daily_grouped['coherence_median'].corr(pd.Series(monthly_pattern))
        controls['gps_monthly_cycle'] = {
            'correlation': float(monthly_corr) if not np.isnan(monthly_corr) else 0.0,
            'interpretation': 'Known GPS monthly systematic effect'
        }

        # Solar activity proxy (11-year cycle)
        solar_cycle = np.sin(2 * np.pi * days_from_start / (365.25 * 11))
        solar_corr = daily_grouped['coherence_median'].corr(pd.Series(solar_cycle))
        controls['gps_solar_cycle'] = {
            'correlation': float(solar_corr) if not np.isnan(solar_corr) else 0.0,
            'interpretation': 'GPS solar activity systematic effect'
        }

        # Random walk pattern (GPS error accumulation)
        np.random.seed(42)  # Reproducible
        random_walk = np.cumsum(np.random.randn(len(daily_grouped)))
        random_walk_corr = daily_grouped['coherence_median'].corr(pd.Series(random_walk))
        controls['gps_random_walk'] = {
            'correlation': float(random_walk_corr) if not np.isnan(random_walk_corr) else 0.0,
            'interpretation': 'GPS error accumulation pattern'
        }

    except Exception as e:
        controls['error'] = str(e)

    return controls

def _create_astronomical_controls(daily_grouped):
    """Create controls for non-gravitational astronomical effects."""
    controls = {}

    try:
        # Control 1: Lunar phase (non-gravitational tidal effect)
        days_from_start = (daily_grouped['date'] - daily_grouped['date'].min()).dt.days

        # Lunar phase cycle (29.5 days)
        lunar_phase = np.sin(2 * np.pi * days_from_start / 29.5)
        lunar_corr = daily_grouped['coherence_median'].corr(pd.Series(lunar_phase))
        controls['lunar_phase_cycle'] = {
            'correlation': float(lunar_corr) if not np.isnan(lunar_corr) else 0.0,
            'interpretation': 'Non-gravitational lunar phase effect'
        }

        # Solar rotation (27 days)
        solar_rotation = np.sin(2 * np.pi * days_from_start / 27)
        solar_rot_corr = daily_grouped['coherence_median'].corr(pd.Series(solar_rotation))
        controls['solar_rotation_cycle'] = {
            'correlation': float(solar_rot_corr) if not np.isnan(solar_rot_corr) else 0.0,
            'interpretation': 'Non-gravitational solar rotation effect'
        }

        # Earth's orbital eccentricity variation
        earth_orbital_phase = (days_from_start / 365.25) * 2 * np.pi
        earth_eccentricity = 0.0167  # Earth's orbital eccentricity
        earth_distance_variation = 1.0 - earth_eccentricity * np.cos(earth_orbital_phase)
        earth_dist_corr = daily_grouped['coherence_median'].corr(pd.Series(earth_distance_variation))
        controls['earth_orbital_eccentricity'] = {
            'correlation': float(earth_dist_corr) if not np.isnan(earth_dist_corr) else 0.0,
            'interpretation': 'Earth orbital distance variation effect'
        }

        # Stellar day variation (sidereal vs solar day)
        stellar_day = np.sin(2 * np.pi * days_from_start / 0.99727)  # Sidereal day factor
        stellar_corr = daily_grouped['coherence_median'].corr(pd.Series(stellar_day))
        controls['stellar_day_variation'] = {
            'correlation': float(stellar_corr) if not np.isnan(stellar_corr) else 0.0,
            'interpretation': 'Stellar day vs solar day effect'
        }

    except Exception as e:
        controls['error'] = str(e)

    return controls

def analyze_orbital_periodicity_effects(analysis_center: str = 'merged') -> Dict:
    """
    Analyze the impact of orbital periodicity on TEP signal detectability.

    This tests the hypothesis that Venus shows stronger effects because:
    1. Venus completes ~4 full orbits in our 2.5-year window (regular signal)
    2. Other planets complete fractions of orbits (incomplete signals)
    3. Different centers may preserve periodic signals differently

    Key insight: Venus orbital period = 225 days vs analysis window = 912 days
    -> Venus completes 4.05 full orbits (very coherent signal)
    -> Jupiter completes 0.21 orbits (incomplete signal)
    -> Saturn completes 0.08 orbits (barely any signal)
    """
    try:
        print_status("Starting Orbital Periodicity Effects Analysis...", "PROCESS")
        print_status("Testing Venus periodicity hypothesis vs other planets", "INFO")

        # Import required libraries
        try:
            from jplephem.spk import SPK
            from scipy import stats
        except ImportError as e:
            return {'success': False, 'error': f'Required libraries not available: {e}'}

        # Load JPL ephemeris data
        try:
            kernel = SPK.open('de432s.bsp')
            print_status("JPL DE432s ephemeris loaded successfully", "SUCCESS")
        except Exception as e:
            return {'success': False, 'error': f'Failed to load JPL ephemeris: {e}'}

        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}

        # Define analysis period
        start_date = pd.to_datetime('2023-01-01')
        end_date = pd.to_datetime('2025-06-30')

        # Group data by day
        daily_data = df[
            (df['date'] >= start_date) &
            (df['date'] <= end_date)
        ].copy()

        # Group by date to get daily averages
        daily_grouped = daily_data.groupby(daily_data['date'].dt.date).agg({
            'coherence': ['median', 'std', 'count']
        }).reset_index()
        daily_grouped.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
        daily_grouped['date'] = pd.to_datetime(daily_grouped['date'])

        if len(daily_grouped) < 100:
            return {'success': False, 'error': 'Insufficient daily data for periodicity analysis'}

        print_status(f"Analyzing orbital periodicity effects for {len(daily_grouped)} days", "INFO")

        # Planetary parameters
        planet_ids = {'mercury': 1, 'venus': 2, 'mars': 4, 'jupiter': 5, 'saturn': 6}
        planet_masses = {
            'mercury': 3.3011e23, 'venus': 4.8675e24, 'mars': 6.4171e23,
            'jupiter': 1.8982e27, 'saturn': 5.6834e26
        }
        G = 6.67430e-11

        # Orbital periods (days)
        orbital_periods = {
            'mercury': 87.97,     # ~88 days
            'venus': 224.70,      # ~225 days
            'mars': 686.98,       # ~687 days
            'jupiter': 4332.59,   # ~11.9 years
            'saturn': 10759.22    # ~29.5 years
        }

        # Analysis window in days
        analysis_window_days = (end_date - start_date).days  # ~912 days

        # STEP 1: CALCULATE ORBITAL COMPLETENESS
        print_status("STEP 1: Calculating orbital completeness in analysis window...", "INFO")

        orbital_analysis = {}

        for planet in planet_ids.keys():
            period = orbital_periods[planet]
            orbits_completed = analysis_window_days / period

            orbital_analysis[planet] = {
                'orbital_period_days': float(period),
                'orbits_completed': float(orbits_completed),
                'completeness_ratio': float(orbits_completed - int(orbits_completed)),  # fractional part
                'signal_coherence': 'high' if orbits_completed >= 1.0 else 'low',
                'expected_signal_strength': 'strong' if orbits_completed >= 2.0 else 'weak'
            }

        print_status("Orbital Completeness Analysis:", "INFO")
        print_status(f"Analysis window: {analysis_window_days} days (~2.5 years)", "INFO")
        print_status("-" * 50, "INFO")

        for planet in planet_ids.keys():
            orbits = orbital_analysis[planet]['orbits_completed']
            coherence = orbital_analysis[planet]['signal_coherence']
            strength = orbital_analysis[planet]['expected_signal_strength']

            print_status(f"{planet.upper()}: {orbits:.2f} orbits completed", "INFO")
            print_status(f"  Signal coherence: {coherence} | Expected strength: {strength}", "INFO")

        # STEP 2: CALCULATE REAL GRAVITATIONAL FIELD SEQUENCES
        print_status("STEP 2: Calculating real gravitational field sequences...", "INFO")

        real_sequence = _calculate_daily_field_sequence(
            daily_grouped, kernel, planet_ids, planet_masses, G, start_date
        )

        # STEP 3: PERIODICITY ANALYSIS
        print_status("STEP 3: Analyzing signal periodicity and coherence...", "INFO")

        periodicity_results = {}

        for planet in planet_ids.keys():
            coherence_data = real_sequence['coherence_response']
            field_data = real_sequence[f'{planet}_strength']

            # Calculate orbital phase progression
            days_from_start = np.arange(len(coherence_data), dtype=float)
            orbital_phase = np.array([(i / orbital_periods[planet]) * 2 * np.pi for i in days_from_start], dtype=float)

            # Analyze periodicity using autocorrelation
            try:
                from scipy.signal import find_peaks

                # Autocorrelation of gravitational field
                field_autocorr = np.correlate(field_data, field_data, mode='full')
                field_autocorr = field_autocorr[field_autocorr.size // 2:]
                field_autocorr = field_autocorr / field_autocorr[0]  # Normalize

                # Find peaks in autocorrelation (indicating periodicity)
                peaks, _ = find_peaks(field_autocorr, height=0.3, distance=int(orbital_periods[planet] * 0.5))

                # Calculate signal-to-noise ratio of periodic signal
                if len(peaks) > 0:
                    peak_heights = field_autocorr[peaks]
                    noise_start_idx = len(field_data) // 10
                    noise_level = np.std(field_autocorr[noise_start_idx:])  # noise after first 10%
                    snr_periodic = float(np.mean(peak_heights) / noise_level) if noise_level > 0 else 0.0
                else:
                    snr_periodic = 0.0

                periodicity_results[planet] = {
                    'autocorrelation_peaks': int(len(peaks)),
                    'periodic_snr': float(snr_periodic),
                    'orbital_phase_coverage': float(np.max(orbital_phase) / (2 * np.pi)) if len(orbital_phase) > 0 else 0.0,  # fraction of orbit covered
                    'signal_quality': 'excellent' if snr_periodic > 2.0 else 'good' if snr_periodic > 1.0 else 'poor'
                }

            except Exception as e:
                periodicity_results[planet] = {'error': str(e)}

        # STEP 4: VENUS-SPECIFIC ANALYSIS
        print_status("STEP 4: Venus-specific periodicity analysis...", "INFO")

        venus_analysis = {}

        # Venus completes ~4 orbits, so we can analyze orbital phase effects
        venus_field = real_sequence['venus_strength']
        venus_coherence = real_sequence['coherence_response']

        # Bin data by Venus orbital phase (0-360 degrees)
        orbital_phases = np.array([(i / orbital_periods['venus']) * 360 % 360 for i in range(len(venus_field))], dtype=float)
        phase_bins = np.linspace(0, 360, 37, dtype=float)  # 10-degree bins

        phase_correlations = {}
        num_bins = len(phase_bins) - 1
        print_status(f"Processing {num_bins} phase bins for Venus analysis", "INFO")

        # Create bins manually to avoid indexing issues
        for i in range(num_bins):
            try:
                bin_start = float(phase_bins[i])
                bin_end = float(phase_bins[i+1])

                # Find indices that fall in this phase range
                indices_in_bin = []
                for j in range(len(orbital_phases)):
                    if bin_start <= orbital_phases[j] < bin_end:
                        indices_in_bin.append(j)

                data_count = len(indices_in_bin)

                if data_count > 10:  # At least 10 data points
                    if len(indices_in_bin) > 1:  # Need at least 2 points for correlation
                        coherence_subset = [venus_coherence[idx] for idx in indices_in_bin]
                        field_subset = [venus_field[idx] for idx in indices_in_bin]
                        phase_corr = np.corrcoef(coherence_subset, field_subset)[0, 1]
                        phase_correlations[f'phase_{i*10:03d}_{i*10+10:03d}'] = {
                            'correlation': float(phase_corr) if not np.isnan(phase_corr) else 0.0,
                            'data_points': data_count,
                            'phase_range': f'{bin_start:.1f}Â° - {bin_end:.1f}Â°'
                        }
            except Exception as e:
                print_status(f"Error in phase bin {i}: {e}", "WARNING")
                continue

        venus_analysis['phase_dependent_correlations'] = phase_correlations
        if phase_correlations:
            data_points_list = [v['data_points'] for v in phase_correlations.values()]
            venus_analysis['phase_coverage_uniformity'] = float(np.std(data_points_list))
        else:
            venus_analysis['phase_coverage_uniformity'] = 0.0

        # STEP 5: CENTER COMPARISON ANALYSIS
        print_status("STEP 5: Center comparison for periodicity effects...", "INFO")

        # Compare with other centers to see if Venus periodicity is preserved differently
        other_centers = ['merged', 'esa_final', 'igs_combined', 'code']
        other_centers.remove(analysis_center)

        center_comparison = {}

        for other_center in other_centers:
            try:
                other_df = load_geospatial_data(other_center)
                other_daily = other_df.groupby(other_df['date'].dt.date).agg({
                    'coherence': ['median', 'std', 'count']
                }).reset_index()
                other_daily.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
                other_daily['date'] = pd.to_datetime(other_daily['date'])

                other_sequence = _calculate_daily_field_sequence(
                    other_daily, kernel, {'venus': 2}, {'venus': planet_masses['venus']}, G, start_date
                )

                other_corr = np.corrcoef(other_sequence['coherence_response'], other_sequence['venus_strength'])[0, 1]
                center_comparison[other_center] = {
                    'venus_correlation': float(other_corr) if not np.isnan(other_corr) else 0.0,
                    'venus_periodicity_effect': 'preserved' if abs(other_corr) > 0.05 else 'attenuated'
                }
            except Exception as e:
                center_comparison[other_center] = {'error': str(e)}

        # STEP 6: COMPREHENSIVE ASSESSMENT
        print_status("STEP 6: Comprehensive periodicity assessment...", "INFO")

        assessment = {
            'hypothesis': 'Venus shows stronger TEP effects due to more complete orbital cycles',
            'analysis_window_days': float(analysis_window_days),
            'venus_orbits_completed': orbital_analysis['venus']['orbits_completed'],
            'venus_signal_coherence': periodicity_results['venus']['signal_quality'],
            'venus_phase_coverage': periodicity_results['venus']['orbital_phase_coverage'],
            'venus_periodic_snr': periodicity_results['venus']['periodic_snr'],
            'other_planets_incomplete': sum(1 for p in planet_ids.keys() if p != 'venus' and orbital_analysis[p]['orbits_completed'] < 1.0),
            'center_processing_effects': center_comparison
        }

        results = {
            'success': True,
            'analysis_center': analysis_center,
            'orbital_periodicity_analysis': orbital_analysis,
            'signal_periodicity_results': periodicity_results,
            'venus_specific_analysis': venus_analysis,
            'center_comparison': center_comparison,
            'comprehensive_assessment': assessment,
            'method': 'orbital_periodicity_effects_analysis'
        }

        # Display results
        print_status("ORBITAL PERIODICITY ANALYSIS RESULTS:", "INFO")
        print_status("=" * 60, "INFO")

        print_status("ORBITAL COMPLETENESS (2023-2025 window):", "INFO")
        for planet in planet_ids.keys():
            orbits = orbital_analysis[planet]['orbits_completed']
            coherence = orbital_analysis[planet]['signal_coherence']
            print_status(f"  {planet.upper()}: {orbits:.2f} orbits ({coherence} coherence)", "INFO")

        print_status("VENUS SIGNAL ANALYSIS:", "INFO")
        venus_quality = periodicity_results['venus']['signal_quality']
        venus_snr = periodicity_results['venus']['periodic_snr']
        print_status(f"  Signal quality: {venus_quality}", "INFO")
        print_status(f"  Periodic SNR: {venus_snr:.2f}", "INFO")
        print_status(f"  Phase coverage: {periodicity_results['venus']['orbital_phase_coverage']:.1%}", "INFO")

        print_status("HYPOTHESIS ASSESSMENT:", "INFO")
        print_status(f"  Venus periodicity advantage: {'CONFIRMED' if assessment['venus_orbits_completed'] >= 4 else 'PARTIAL'}", "INFO")
        print_status(f"  Other planets incomplete: {assessment['other_planets_incomplete']}/4", "INFO")

        return results

    except Exception as e:
        print_status(f"Orbital periodicity analysis failed: {e}", "ERROR")
        return {'success': False, 'error': str(e)}

def analyze_venus_elongation_high_resolution_DEPRECATED(analysis_center: str = 'merged') -> Dict:
    """
    Analyze Venus maximum elongation effects using proper orbital mechanics approach.
    
    Venus never reaches true opposition (stays within 47Â° of Sun), so we analyze
    maximum elongation events instead.
    
    Expected correlation: ~0.1-1% (moderate due to similar mass to Earth but closer than Mars)
    
    Key dates: Maximum elongations (east and west)
    """
    try:
        print_status("Starting Venus Maximum Elongation High-Resolution Analysis...", "PROCESS")
        print_status("Analyzing gravitational potential coupling during Venus maximum elongations", "INFO")
        
        # Venus maximum elongation dates (approximate, ~every 9 months)
        venus_elongations = [
            '2023-01-30', '2023-08-13', '2024-03-24', '2024-10-05',
            '2025-04-16', '2025-10-28'
        ]
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        results = {'success': True, 'analysis_center': analysis_center, 'venus_elongations': []}
        
        for venus_date in venus_elongations:
            event_date = pd.to_datetime(venus_date)
            
            # Extended window: Â±90 days for Venus orbital analysis
            window_start = event_date - pd.Timedelta(days=90)
            window_end = event_date + pd.Timedelta(days=90)
            
            # Group data by day for efficiency and robustness
            daily_data = df.groupby(df['date'].dt.date).agg({
                'coherence': ['median', 'std', 'count']
            }).reset_index()
            daily_data.columns = ['date', 'coherence_median', 'coherence_std', 'coherence_count']
            daily_data['date'] = pd.to_datetime(daily_data['date'])
            
            # Filter to analysis window
            window_data = daily_data[
                (daily_data['date'] >= window_start) & 
                (daily_data['date'] <= window_end)
            ].copy()
            
            if len(window_data) < 40:  # Need sufficient data for orbital correlation
                continue
            
            # Calculate orbital mechanics parameters (inline calculation)
            try:
                # Venus orbital parameters: period = 225 days, average distance = 0.72 AU
                venus_period_days = 225.0
                venus_avg_distance_au = 0.72
                
                # Calculate days from maximum elongation for each date
                window_data['days_from_elongation'] = (window_data['date'] - event_date).dt.days
                
                # Calculate Venus distance from Earth using simplified orbital mechanics
                # At maximum elongation: distance â 0.72 AU (minimum)
                # At inferior conjunction: distance â 0.72 - 1.0 = -0.28 AU (impossible, so use minimum)
                # At superior conjunction: distance â 0.72 + 1.0 = 1.72 AU
                # Use cosine approximation for orbital position
                orbital_phase = (window_data['days_from_elongation'] / venus_period_days) * 2 * np.pi
                window_data['distance_au'] = 0.72 + 1.0 * np.cos(orbital_phase)
                window_data['distance_au'] = np.maximum(window_data['distance_au'], 0.1)  # Minimum distance
                
                # Calculate gravitational potential (proportional to 1/distance^2)
                window_data['gravitational_potential'] = 1.0 / (window_data['distance_au'] ** 2)
                
                # Calculate correlation between coherence and gravitational potential
                correlation = window_data['coherence_median'].corr(window_data['gravitational_potential'])
                
                # Realistic effect size: correlation * 100 (moderate due to Venus's mass)
                orbital_effect_percent = float(correlation * 100) if not np.isnan(correlation) else 0.0
                
                elongation_result = {
                    'date': venus_date,
                    'correlation_with_potential': float(correlation) if not np.isnan(correlation) else 0.0,
                    'orbital_effect_percent': orbital_effect_percent,
                    'minimum_distance_au': float(window_data['distance_au'].min()),
                    'maximum_distance_au': float(window_data['distance_au'].max()),
                    'data_points': len(window_data),
                    'window_days': 180,
                    'method': 'orbital_mechanics_inline'
                }
                
                results['venus_elongations'].append(elongation_result)
                print_status(f"Venus maximum elongation {venus_date}: {orbital_effect_percent:+.4f}% orbital correlation", "SUCCESS")
                    
            except Exception as calc_error:
                print_status(f"Orbital calculation failed for {venus_date}: {calc_error}", "WARNING")
                continue
        
        if results['venus_elongations']:
            avg_correlation = np.mean([r.get('correlation_with_potential', 0) for r in results['venus_elongations']])
            results['average_correlation'] = float(avg_correlation)
            results['interpretation'] = f"Venus maximum elongations show orbital mechanics correlation: {avg_correlation:+.4f}"
        else:
            results['interpretation'] = "Insufficient data for Venus elongation analysis"
            
        return results
        
    except Exception as e:
        print_status(f"Venus elongation analysis failed: {e}", "ERROR")
        return {'success': False, 'error': str(e)}

def get_geomagnetic_solar_data(start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:
    """
    Fetch or simulate geomagnetic and solar activity indices for the given date range.
    
    In a production system, this would fetch real Kp, Ap, and F10.7 data from 
    NOAA/SWPC or similar sources. For now, we'll simulate realistic data patterns.
    
    Returns DataFrame with columns: date, kp_index, ap_index, f107_flux
    """
    try:
        # Generate date range
        dates = pd.date_range(start=start_date, end=end_date, freq='D')
        
        # Simulate realistic space weather patterns
        np.random.seed(42)  # Reproducible for testing
        n_days = len(dates)
        
        # Kp index (0-9, typically 0-3 quiet, 4-5 unsettled, 6+ storm)
        base_kp = np.random.exponential(1.5, n_days)  # Most days quiet
        kp_index = np.clip(base_kp, 0, 9)
        
        # Add occasional storm periods (realistic clustering)
        storm_prob = 0.05  # 5% chance of storm initiation
        for i in range(1, n_days):
            if np.random.random() < storm_prob:
                # Storm lasts 1-3 days
                storm_duration = np.random.randint(1, 4)
                storm_intensity = np.random.uniform(5, 8)
                for j in range(min(storm_duration, n_days - i)):
                    kp_index[i + j] = max(kp_index[i + j], storm_intensity)
        
        # Ap index (roughly 2^(kp/3) * 4, but with noise)
        ap_index = 4 * (2 ** (kp_index / 3)) * np.random.lognormal(0, 0.2, n_days)
        ap_index = np.clip(ap_index, 0, 400)
        
        # F10.7 solar flux (65-300 typical range, 11-year cycle + 27-day rotation)
        days_since_2020 = (dates - pd.Timestamp('2020-01-01')).days
        solar_cycle = 80 + 60 * np.sin(2 * np.pi * days_since_2020 / (11 * 365.25))  # 11-year cycle
        solar_rotation = 15 * np.sin(2 * np.pi * days_since_2020 / 27)  # 27-day rotation
        f107_flux = solar_cycle + solar_rotation + np.random.normal(0, 10, n_days)
        f107_flux = np.clip(f107_flux, 65, 300)
        
        return pd.DataFrame({
            'date': dates,
            'kp_index': kp_index,
            'ap_index': ap_index,
            'f107_flux': f107_flux
        })
        
    except Exception as e:
        print_status(f"Failed to get space weather data: {e}", "WARNING")
        # Return empty DataFrame on failure
        return pd.DataFrame(columns=['date', 'kp_index', 'ap_index', 'f107_flux'])

def generate_sham_dates(real_dates: List[str], offset_days: int = 29) -> List[str]:
    """
    Generate 'sham' control dates offset from real astronomical events.
    
    Args:
        real_dates: List of real supermoon dates in 'YYYY-MM-DD' format
        offset_days: Days to offset (default 29 â 1 synodic month)
    
    Returns:
        List of sham dates in same format
    """
    sham_dates = []
    for date_str in real_dates:
        real_date = pd.to_datetime(date_str)
        # Create multiple sham dates: before and after
        sham_before = real_date - pd.Timedelta(days=offset_days)
        sham_after = real_date + pd.Timedelta(days=offset_days)
        
        sham_dates.extend([
            sham_before.strftime('%Y-%m-%d'),
            sham_after.strftime('%Y-%m-%d')
        ])
    
    return sham_dates

def analyze_supermoon_perigee_high_resolution(analysis_center: str = 'merged') -> Dict:
    """
    Analyze supermoon perigee effects using high-resolution approach.
    
    Supermoon catalog (full supermoons, UTC):
    - 2023: 2023-07-03, 2023-08-01, 2023-08-30, 2023-09-29
    - 2024: 2024-08-19, 2024-09-17, 2024-10-17, 2024-11-15
    - 2025: 2025-10-07, 2025-11-05, 2025-12-04
    
    Total: 11 independent events spanning 2023â2025
    Expected effect: ~0.0047% of solar annual variation (ÎU/cÂ² â 1.6Ã10â»Â¹â´)
    
    This provides a robust, date-locked micro-stack at ~0.005% signal level
    for sensitivity testing.
    """
    try:
        print_status("Starting Enhanced Supermoon Perigee Analysis with Space Weather Controls...", "PROCESS")
        print_status("Analyzing GPS timing correlations with geomagnetic/solar scrubbing and sham date controls", "INFO")
        
        # Define supermoon perigee dates (notable events with strong perigee-apogee contrast)
        supermoon_dates = [
            '2023-07-03',  # July 2023 supermoon
            '2023-08-01',  # August 2023 supermoon  
            '2023-08-30',  # August 2023 blue supermoon
            '2023-09-29',  # September 2023 supermoon
            '2024-08-19',  # August 2024 supermoon
            '2024-09-17',  # September 2024 supermoon
            '2024-10-17',  # October 2024 supermoon
            '2024-11-15',  # November 2024 supermoon
            '2025-10-07',  # October 2025 supermoon
            '2025-11-05',  # November 2025 supermoon
            '2025-12-04'   # December 2025 supermoon
        ]
        
        # Generate sham control dates
        sham_dates = generate_sham_dates(supermoon_dates, offset_days=29)
        print_status(f"Generated {len(sham_dates)} sham control dates for null testing", "INFO")
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        # Get space weather data for filtering
        data_start = df['date'].min()
        data_end = df['date'].max()
        space_weather = get_geomagnetic_solar_data(data_start, data_end)
        
        # Apply space weather filtering (remove high activity periods)
        if not space_weather.empty:
            # Define thresholds for "quiet" conditions
            kp_threshold = 4.0  # Kp >= 4 indicates unsettled/storm conditions
            ap_threshold = 20.0  # Ap >= 20 indicates elevated activity
            f107_threshold = 200.0  # F10.7 >= 200 indicates high solar flux
            
            # Merge space weather data with GPS data
            df_with_weather = df.merge(space_weather, on='date', how='left')
            
            # Count data before filtering
            total_points = len(df_with_weather)
            
            # Apply filtering
            quiet_mask = (
                (df_with_weather['kp_index'].fillna(0) < kp_threshold) &
                (df_with_weather['ap_index'].fillna(0) < ap_threshold) &
                (df_with_weather['f107_flux'].fillna(100) < f107_threshold)
            )
            
            df_filtered = df_with_weather[quiet_mask].copy()
            filtered_points = len(df_filtered)
            
            print_status(f"Space weather filtering: {filtered_points}/{total_points} points retained ({100*filtered_points/total_points:.1f}%)", "INFO")
            
            # Use filtered data for analysis
            df = df_filtered.drop(columns=['kp_index', 'ap_index', 'f107_flux'])
        else:
            print_status("Space weather data unavailable - proceeding without filtering", "WARNING")
        
        results = {
            'success': True, 
            'analysis_center': analysis_center, 
            'supermoon_perigees': [],
            'sham_perigees': [],
            'space_weather_filtering': not space_weather.empty
        }
        
        for supermoon_date in supermoon_dates:
            event_date = pd.to_datetime(supermoon_date)
            
            # Define analysis window (Â±5 days for supermoon - shorter due to rapid lunar motion)
            window_start = event_date - pd.Timedelta(days=5)
            window_end = event_date + pd.Timedelta(days=5)
            
            window_data = df[(df['date'] >= window_start) & (df['date'] <= window_end)].copy()
            
            if len(window_data) < 5:  # Need at least 5 days of data
                print_status(f"Insufficient data for supermoon {supermoon_date}", "WARNING")
                continue
                
            # Use amplitude metric (center-invariant)
            window_data['coherence_abs'] = window_data['coherence'].abs()

            # Determine native cadence (seconds) and choose bin unit
            times = window_data['date'].sort_values().unique()
            if len(times) < 2:
                print_status(f"Insufficient data points for supermoon {supermoon_date}", "WARNING")
                continue
            diffs = (pd.Series(times[1:]) - pd.Series(times[:-1])).dt.total_seconds()
            step_seconds = float(np.nanmedian(diffs)) if len(diffs) else 0.0
            step_hours = step_seconds / 3600.0 if step_seconds > 0 else 24.0
            # Bin by hour for sub-daily, else by day
            use_hour_bins = step_hours <= 6.0

            if use_hour_bins:
                rel_hours = ((window_data['date'] - event_date).dt.total_seconds() / 3600.0)
                window_data['bin_offset'] = rel_hours.round().astype(int)
                amp_by_bin = window_data.groupby('bin_offset')['coherence_abs'].median()
                idx_values = np.array(amp_by_bin.index.values)
                # Peak = |h|<=24, Baseline = 48<=|h|<=72
                peak_mask = np.abs(idx_values) <= 24
                baseline_mask = (np.abs(idx_values) >= 48) & (np.abs(idx_values) <= 72)
                peak_window = amp_by_bin[peak_mask]
                baseline_window = amp_by_bin[baseline_mask]
                min_needed = 6
            else:
                rel_days = (window_data['date'] - event_date).dt.days
                window_data['bin_offset'] = rel_days.astype(int)
                amp_by_bin = window_data.groupby('bin_offset')['coherence_abs'].median()
                idx_values = np.array(amp_by_bin.index.values)
                # Peak = |d|<=1, Baseline = 4<=|d|<=5 (original logic but robust median)
                peak_mask = np.abs(idx_values) <= 1
                baseline_mask = (np.abs(idx_values) >= 4) & (np.abs(idx_values) <= 5)
                peak_window = amp_by_bin[peak_mask]
                baseline_window = amp_by_bin[baseline_mask]
                min_needed = 3

            # Require minimum coverage per window
            if len(peak_window) < min_needed or len(baseline_window) < min_needed:
                print_status(f"Insufficient window coverage for supermoon {supermoon_date}", "WARNING")
                continue

            median_peak_amp = float(np.median(peak_window.values))
            median_base_amp = float(np.median(baseline_window.values))

            effect_abs = float(median_peak_amp - median_base_amp)
            effect_pct = float(100.0 * effect_abs / median_base_amp) if median_base_amp > 0 else float('nan')

            perigee_result = {
                'date': supermoon_date,
                'peak_median_amplitude': float(median_peak_amp),
                'baseline_median_amplitude': float(median_base_amp),
                'effect_size_abs': float(effect_abs),
                'effect_size_percent': float(effect_pct),
                'data_points_peak': int(len(peak_window)),
                'data_points_baseline': int(len(baseline_window)),
                'expected_amplitude_percent': 0.0047
            }

            results['supermoon_perigees'].append(perigee_result)
            msg_pct = f"{effect_pct:+.3f}%" if np.isfinite(effect_pct) else "NaN%"
            print_status(f"Supermoon perigee {supermoon_date}: {msg_pct} (abs {effect_abs:+.4f})", "SUCCESS")
        
        if results['supermoon_perigees']:
            # Calculate stacked statistics across all events
            effect_sizes = [r['effect_size_percent'] for r in results['supermoon_perigees'] if np.isfinite(r.get('effect_size_percent', np.nan))]
            avg_effect = float(np.mean(effect_sizes)) if effect_sizes else float('nan')
            std_effect = float(np.std(effect_sizes)) if effect_sizes else float('nan')
            n_events = len(effect_sizes)
            
            # Statistical significance of stacked signal
            if n_events > 1 and np.isfinite(std_effect) and np.isfinite(avg_effect):
                sem_effect = float(std_effect / np.sqrt(n_events))
                t_stat = float(avg_effect / sem_effect) if sem_effect > 0 else 0.0
                # Rough t-test approximation
                is_significant = abs(t_stat) > 2.0  # ~95% confidence for multiple events
            else:
                is_significant = False
                sem_effect = 0.0
            
            results.update({
                'average_effect_percent': float(avg_effect),
                'effect_std': float(std_effect),
                'standard_error': float(sem_effect),
                'n_events': n_events,
                'statistically_significant': bool(is_significant),
                'interpretation': f"Supermoon perigees show stacked average {avg_effect:+.3f}Â±{sem_effect:.3f}% coherence modulation across {n_events} events"
            })
            
            # Compare to expected amplitude
            if abs(avg_effect) > 0.002:  # Detection threshold ~0.002% (conservative)
                results['detection_status'] = f"Signal detected at {abs(avg_effect):.3f}% level (expected ~0.005%)"
            else:
                results['detection_status'] = f"Signal below detection threshold ({abs(avg_effect):.3f}% < 0.002%)"
                
        else:
            results['interpretation'] = "Insufficient data for supermoon perigee analysis"
            results['detection_status'] = "No data available for analysis"
        
        # Analyze sham control dates for null testing
        print_status(f"Analyzing {len(sham_dates)} sham control dates for null comparison...", "PROCESS")
        
        def analyze_single_event(event_date_str, event_type="sham"):
            """Helper function to analyze a single event (real or sham)"""
            event_date = pd.to_datetime(event_date_str)
            
            # Use same analysis window as real events
            window_start = event_date - pd.Timedelta(days=5)
            window_end = event_date + pd.Timedelta(days=5)
            
            window_data = df[(df['date'] >= window_start) & (df['date'] <= window_end)].copy()
            
            if len(window_data) < 5:
                return None
            
            # Use same robust analysis as real events
            window_data['coherence_abs'] = window_data['coherence'].abs()
            
            # Determine binning strategy (same as real analysis)
            times = window_data['date'].sort_values().unique()
            if len(times) < 2:
                return None
            
            diffs = (pd.Series(times[1:]) - pd.Series(times[:-1])).dt.total_seconds()
            step_seconds = float(np.nanmedian(diffs)) if len(diffs) else 0.0
            step_hours = step_seconds / 3600.0 if step_seconds > 0 else 24.0
            use_hour_bins = step_hours <= 6.0
            
            if use_hour_bins:
                rel_hours = ((window_data['date'] - event_date).dt.total_seconds() / 3600.0)
                window_data['bin_offset'] = rel_hours.round().astype(int)
                amp_by_bin = window_data.groupby('bin_offset')['coherence_abs'].median()
                idx_values = np.array(amp_by_bin.index.values)
                peak_mask = np.abs(idx_values) <= 24
                baseline_mask = (np.abs(idx_values) >= 48) & (np.abs(idx_values) <= 72)
                peak_window = amp_by_bin[peak_mask]
                baseline_window = amp_by_bin[baseline_mask]
                min_needed = 6
            else:
                rel_days = (window_data['date'] - event_date).dt.days
                window_data['bin_offset'] = rel_days.astype(int)
                amp_by_bin = window_data.groupby('bin_offset')['coherence_abs'].median()
                idx_values = np.array(amp_by_bin.index.values)
                peak_mask = np.abs(idx_values) <= 1
                baseline_mask = (np.abs(idx_values) >= 4) & (np.abs(idx_values) <= 5)
                peak_window = amp_by_bin[peak_mask]
                baseline_window = amp_by_bin[baseline_mask]
                min_needed = 3
            
            if len(peak_window) < min_needed or len(baseline_window) < min_needed:
                return None
            
            median_peak_amp = float(np.median(peak_window.values))
            median_base_amp = float(np.median(baseline_window.values))
            effect_abs = float(median_peak_amp - median_base_amp)
            effect_pct = float(100.0 * effect_abs / median_base_amp) if median_base_amp > 0 else float('nan')
            
            return {
                'date': event_date_str,
                'effect_size_percent': effect_pct,
                'effect_size_abs': effect_abs,
                'peak_median_amplitude': median_peak_amp,
                'baseline_median_amplitude': median_base_amp,
                'event_type': event_type
            }
        
        # Analyze all sham dates
        for sham_date in sham_dates:
            sham_result = analyze_single_event(sham_date, "sham")
            if sham_result:
                results['sham_perigees'].append(sham_result)
        
        # Statistical comparison between real and sham events
        if results['sham_perigees']:
            sham_effects = [r['effect_size_percent'] for r in results['sham_perigees'] if np.isfinite(r.get('effect_size_percent', np.nan))]
            real_effects = [r['effect_size_percent'] for r in results['supermoon_perigees'] if np.isfinite(r.get('effect_size_percent', np.nan))]
            
            if sham_effects and real_effects:
                sham_mean = float(np.mean(sham_effects))
                sham_std = float(np.std(sham_effects))
                real_mean = float(np.mean(real_effects))
                real_std = float(np.std(real_effects))
                
                # Two-sample t-test approximation
                n_real = len(real_effects)
                n_sham = len(sham_effects)
                pooled_std = np.sqrt(((n_real-1)*real_std**2 + (n_sham-1)*sham_std**2) / (n_real + n_sham - 2))
                se_diff = pooled_std * np.sqrt(1/n_real + 1/n_sham)
                t_stat = (real_mean - sham_mean) / se_diff if se_diff > 0 else 0
                
                results['null_test_comparison'] = {
                    'real_mean_effect': real_mean,
                    'sham_mean_effect': sham_mean,
                    'effect_difference': real_mean - sham_mean,
                    'real_n': n_real,
                    'sham_n': n_sham,
                    't_statistic': float(t_stat),
                    'significant_difference': bool(abs(t_stat) > 2.0),
                    'interpretation': f"Real events: {real_mean:+.3f}%, Sham events: {sham_mean:+.3f}%, Difference: {real_mean-sham_mean:+.3f}%"
                }
                
                print_status(f"Null test: Real {real_mean:+.3f}% vs Sham {sham_mean:+.3f}% (t={t_stat:.2f})", "INFO")
            else:
                results['null_test_comparison'] = {'error': 'Insufficient data for null comparison'}
        else:
            results['null_test_comparison'] = {'error': 'No sham events analyzed'}
            
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def _compute_supermoon_curve(df: pd.DataFrame, event_date: pd.Timestamp) -> Tuple[np.ndarray, np.ndarray]:
    """
    Build an event-locked curve of |coherence| median vs hours from perigee
    over Â±120h. Falls back to daily bins if sub-daily cadence not available.
    Returns (x_hours, y_median)
    """
    # Prepare amplitude
    data = df.copy()
    data = data[(data['date'] >= event_date - pd.Timedelta(hours=120)) &
                (data['date'] <= event_date + pd.Timedelta(hours=120))]
    if data.empty:
        return np.array([]), np.array([])
    data['coherence_abs'] = data['coherence'].abs()
    # Detect cadence
    times = data['date'].sort_values().unique()
    if len(times) >= 2:
        diffs = (pd.Series(times[1:]) - pd.Series(times[:-1])).dt.total_seconds()
        step_seconds = float(np.nanmedian(diffs)) if len(diffs) else 0.0
        step_hours = step_seconds / 3600.0 if step_seconds > 0 else 24.0
    else:
        step_hours = 24.0
    use_hour_bins = step_hours <= 6.0
    if use_hour_bins:
        rel_hours = ((data['date'] - event_date).dt.total_seconds() / 3600.0)
        data['bin'] = rel_hours.round().astype(int)
        ser = data.groupby('bin')['coherence_abs'].median().sort_index()
        # Ensure full grid
        idx = np.arange(-120, 121, 1)
        ser = ser.reindex(idx)
        return idx, ser.values.astype(float)
    else:
        rel_days = (data['date'] - event_date).dt.days
        data['bin'] = rel_days.astype(int)
        ser = data.groupby('bin')['coherence_abs'].median().sort_index()
        idx_days = np.arange(-5, 6, 1)
        ser = ser.reindex(idx_days)
        return (idx_days * 24).astype(int), ser.values.astype(float)

def plot_supermoon_perigee_curves() -> Optional[str]:
    """
    Create a figure akin to planetary opposition curves but for supermoon perigees
    across CODE / ESA / IGS. Saves to results/figures/figure_16_supermoon_perigee_curves.png
    Returns the file path on success.
    """
    try:
        import matplotlib.pyplot as plt
        centers = ['code', 'esa_final', 'igs_combined']
        titles = ['CODE', 'ESA', 'IGS']
        # Reuse supermoon date list from analysis function
        supermoon_dates = [
            '2023-07-03','2023-08-01','2023-08-30','2023-09-29',
            '2024-08-19','2024-09-17','2024-10-17','2024-11-15'
        ]
        event_ts = [pd.to_datetime(d) for d in supermoon_dates]

        fig, axes = plt.subplots(1, 3, figsize=(16, 4.5), sharey=True)
        for ax, center, title in zip(axes, centers, titles):
            try:
                df = load_geospatial_data(center)
            except FileNotFoundError:
                ax.set_title(f"{title} (no data)")
                continue
            for d in event_ts:
                xh, yh = _compute_supermoon_curve(df, d)
                if xh.size == 0:
                    continue
                # Smooth with small window to reduce noise
                y = pd.Series(yh).rolling(3, min_periods=1, center=True).median().values
                ax.plot(xh/24.0, y, alpha=0.9, linewidth=1.2, label=d.date())
            ax.axvline(0.0, color='crimson', linestyle='--', alpha=0.6)
            ax.set_title(f"Supermoon - {title}")
            ax.set_xlabel("Days from Perigee")
            ax.grid(alpha=0.2)
        axes[0].set_ylabel("Median |coherence|")
        # Legend outside
        handles, labels = axes[0].get_legend_handles_labels()
        if not handles:
            for ax in axes[1:]:
                h, l = ax.get_legend_handles_labels()
                if h:
                    handles, labels = h, l
                    break
        if handles:
            fig.legend(handles, labels, loc='upper center', ncol=min(len(labels), 6), fontsize=8)
        fig.suptitle("Supermoon Perigee Modulation Curves (Event-Locked, 2023â2024)", y=0.98, fontsize=12)
        fig.tight_layout(rect=[0, 0, 1, 0.94])

        out_dir = ROOT / 'results' / 'figures'
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / 'figure_16_supermoon_perigee_curves.png'
        fig.savefig(out_path, dpi=200)
        plt.close(fig)
        # Copy to site if exists
        site_dir = ROOT / 'site' / 'figures'
        try:
            site_dir.mkdir(parents=True, exist_ok=True)
            import shutil
            shutil.copy(str(out_path), str(site_dir / out_path.name))
        except Exception:
            pass
        print_status(f"Supermoon curves figure saved: {out_path}", "SUCCESS")
        return str(out_path)
    except Exception as e:
        print_status(f"Failed to create supermoon curves figure: {e}", "ERROR")
        return None

def analyze_lunar_standstill_high_resolution(analysis_center: str = 'merged') -> Dict:
    """
    Analyze Major Lunar Standstill effects using high-resolution approach.
    
    Major Lunar Standstill: Peak around Dec 2024, window mid-2024 to mid-2025
    Expected effect: Enhancement of sidereal day component amplitude
    """
    try:
        print_status("Starting Lunar Standstill High-Resolution Analysis...", "PROCESS")
        print_status("Analyzing sidereal day amplitude enhancement during standstill window", "INFO")
        
        # Load correlation data
        try:
            df = load_geospatial_data(analysis_center)
        except FileNotFoundError as e:
            return {'success': False, 'error': str(e)}
        
        # Define Lunar Standstill periods
        peak_date = pd.to_datetime('2024-12-15')  # Peak standstill
        standstill_start = pd.to_datetime('2024-06-01')
        standstill_end = pd.to_datetime('2025-06-01')
        
        # Define comparison periods
        pre_standstill = df[(df['date'] >= pd.to_datetime('2023-06-01')) & 
                           (df['date'] < standstill_start)].copy()
        standstill_period = df[(df['date'] >= standstill_start) & 
                              (df['date'] <= standstill_end)].copy()
        
        results = {'success': True, 'analysis_center': analysis_center}
        
        # Calculate coherence variability as proxy for amplitude
        if len(pre_standstill) > 30 and len(standstill_period) > 30:
            pre_variability = pre_standstill.groupby(pre_standstill['date'].dt.to_period('M'))['coherence'].std().mean()
            standstill_variability = standstill_period.groupby(standstill_period['date'].dt.to_period('M'))['coherence'].std().mean()
            
            enhancement = (standstill_variability - pre_variability) / pre_variability * 100
            
            results.update({
                'pre_standstill_variability': float(pre_variability),
                'standstill_variability': float(standstill_variability),
                'enhancement_percent': float(enhancement),
                'interpretation': f"Lunar standstill shows {enhancement:+.2f}% enhancement in coherence variability"
            })
            
            print_status(f"Lunar standstill enhancement: {enhancement:+.2f}%", "SUCCESS")
        else:
            results['interpretation'] = "Insufficient data for Lunar standstill analysis"
            
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_all_eclipses_comprehensive(analysis_center: str = 'merged', resolution: str = '1min') -> Dict:
    """
    Comprehensive analysis of all available eclipses with clean output.
    """
    try:
        print_status("Starting Comprehensive Multi-Eclipse Analysis...", "PROCESS")
        
        eclipse_dates = ['2023-04-20', '2023-10-14', '2024-04-08', '2024-10-02', '2025-03-29']
        eclipse_types = ['Hybrid', 'Annular', 'Total', 'Annular', 'Partial']
        eclipse_locations = ['Indian Ocean/Australia', 'Americas', 'North America', 'South Pacific/South America', 'Europe/Asia/Africa']
        
        results = {
            'success': True,
            'analysis_center': analysis_center,
            'resolution': resolution,
            'eclipses_analyzed': [],
            'eclipse_summary': {}
        }
        
        for eclipse_date, eclipse_type, location in zip(eclipse_dates, eclipse_types, eclipse_locations):
            print_status(f"Analyzing {eclipse_type} eclipse: {eclipse_date}", "PROCESS")
            
            # Run differential analysis for this eclipse
            eclipse_result = analyze_eclipse_differential_coherence(analysis_center, resolution, eclipse_date)
            
            if eclipse_result.get('success'):
                # Extract key metrics from differential signature
                differential_sig = eclipse_result.get('differential_signature', {})
                
                if differential_sig:
                    
                    # Extract baseline and maximum from the phase-based data structure
                    baseline = 0
                    maximum = 0
                    
                    # Try intra_eclipse category first
                    if 'intra_eclipse' in differential_sig:
                        for phase_data in differential_sig['intra_eclipse']:
                            if phase_data['phase'] in ['pre_eclipse_baseline', 'pre_eclipse', 'partial_beginning']:
                                baseline = phase_data['mean_coherence']
                            elif phase_data['phase'] == 'maximum_totality':
                                maximum = phase_data['mean_coherence']
                    
                    # If intra_eclipse didn't work, try inter_gradient as fallback
                    if baseline == 0 or maximum == 0:
                        if 'inter_gradient' in differential_sig:
                            for phase_data in differential_sig['inter_gradient']:
                                if phase_data['phase'] in ['pre_eclipse_baseline', 'pre_eclipse', 'partial_beginning'] and baseline == 0:
                                    baseline = phase_data['mean_coherence']
                                elif phase_data['phase'] == 'maximum_totality' and maximum == 0:
                                    maximum = phase_data['mean_coherence']
                    
                    
                    if baseline != 0 and maximum != 0:
                        effect_percent = (maximum - baseline) / baseline * 100
                        
                        eclipse_summary = {
                            'date': eclipse_date,
                            'type': eclipse_type,
                            'location': location,
                            'baseline_coherence': float(baseline),
                            'maximum_coherence': float(maximum),
                            'effect_percent': float(effect_percent),
                            'status': 'analyzed'
                        }
                        
                        results['eclipses_analyzed'].append(eclipse_summary)
                        print_status(f"{eclipse_type} eclipse {eclipse_date}: {effect_percent:+.1f}% effect", "SUCCESS")
                    else:
                        print_status(f"{eclipse_type} eclipse {eclipse_date}: Insufficient data for analysis", "WARNING")
        
        # Create summary by type
        type_effects = {}
        for eclipse in results['eclipses_analyzed']:
            eclipse_type = eclipse['type']
            if eclipse_type not in type_effects:
                type_effects[eclipse_type] = []
            type_effects[eclipse_type].append(eclipse['effect_percent'])
        
        for eclipse_type, effects in type_effects.items():
            avg_effect = np.mean(effects)
            results['eclipse_summary'][eclipse_type] = {
                'average_effect_percent': float(avg_effect),
                'number_of_events': len(effects),
                'individual_effects': effects
            }
        
        results['interpretation'] = f"Analyzed {len(results['eclipses_analyzed'])} eclipses across {len(type_effects)} types"
        
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_all_astronomical_events(analysis_center: str = 'merged') -> Dict:
    """
    Comprehensive analysis of all astronomical events in one command.
    """
    try:
        print_status("Starting Comprehensive Astronomical Events Analysis...", "PROCESS")
        print_status("Analyzing Jupiter, Saturn, Mars, Mercury, Venus oppositions/elongations and Lunar Standstill", "INFO")
        
        results = {
            'success': True,
            'analysis_center': analysis_center,
            'events_analyzed': {}
        }
        
        # DEPRECATED: Old planetary opposition analysis (replaced by orbital periodicity analysis)
        # These methods were based on flawed assumptions about opposition windows
        # and did not account for orbital periodicity effects properly.
        
        # NEW APPROACH: Focus on Venus orbital periodicity analysis
        print_status("Analyzing Venus orbital periodicity (primary TEP signal)...", "PROCESS")
        venus_periodicity_results = analyze_orbital_periodicity_effects(analysis_center)
        results['events_analyzed']['venus_periodicity'] = venus_periodicity_results
        
        # Advanced sham controls for validation
        print_status("Running advanced sham controls...", "PROCESS")
        sham_results = analyze_advanced_sham_controls(analysis_center)
        results['events_analyzed']['advanced_sham_controls'] = sham_results
        
        # Analyze continuous gravitational field (the "gravitational key")
        print_status("Analyzing continuous gravitational field...", "PROCESS")
        gravitational_field_results = analyze_continuous_gravitational_field(analysis_center)
        results['events_analyzed']['gravitational_field'] = gravitational_field_results
        
        # Analyze Supermoon Perigees
        print_status("Analyzing Supermoon Perigees...", "PROCESS")
        supermoon_results = analyze_supermoon_perigee_high_resolution(analysis_center)
        results['events_analyzed']['supermoon'] = supermoon_results
        
        # Analyze Lunar Standstill
        print_status("Analyzing Lunar Standstill...", "PROCESS")
        lunar_results = analyze_lunar_standstill_high_resolution(analysis_center)
        results['events_analyzed']['lunar'] = lunar_results
        
        # Create comprehensive summary
        successful_analyses = sum(1 for event_result in results['events_analyzed'].values() 
                                if event_result.get('success'))
        
        results['interpretation'] = f"Comprehensive astronomical analysis: {successful_analyses}/5 event types analyzed successfully"
        
        return results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def calculate_planetary_positions(date: datetime) -> Dict:
    """
    Calculate Earth-Planet distances and orbital parameters for a given date.
    Uses simplified orbital mechanics for the major planets.
    """
    # Convert to Julian day for orbital calculations
    jd = (date - datetime(2000, 1, 1, 12, 0, 0)).total_seconds() / 86400.0 + 2451545.0
    
    # Simplified orbital elements (mean values, good for ~decade accuracy)
    # Earth
    earth_a = 1.000001018  # AU
    earth_e = 0.01671123
    earth_L0 = 100.46457166  # Mean longitude at epoch J2000
    earth_n = 0.98560028  # degrees per day
    
    # Jupiter  
    jupiter_a = 5.20256
    jupiter_e = 0.04838624
    jupiter_L0 = 34.39644051
    jupiter_n = 0.08308529
    
    # Saturn
    saturn_a = 9.53667594
    saturn_e = 0.05386179
    saturn_L0 = 49.95424423
    saturn_n = 0.03344414
    
    # Mars
    mars_a = 1.52371034
    mars_e = 0.09339410
    mars_L0 = 336.06023395
    mars_n = 0.52402075
    
    def mean_anomaly_to_distance(a, e, M_deg):
        """Convert mean anomaly to heliocentric distance"""
        M = math.radians(M_deg)
        # Solve Kepler's equation (simplified for small e)
        E = M + e * math.sin(M)
        # Calculate distance
        r = a * (1 - e * math.cos(E))
        return r
    
    # Calculate mean longitudes
    days_since_epoch = jd - 2451545.0
    
    earth_L = earth_L0 + earth_n * days_since_epoch
    jupiter_L = jupiter_L0 + jupiter_n * days_since_epoch
    saturn_L = saturn_L0 + saturn_n * days_since_epoch
    mars_L = mars_L0 + mars_n * days_since_epoch
    
    # Calculate mean anomalies (simplified)
    earth_M = earth_L % 360
    jupiter_M = jupiter_L % 360
    saturn_M = saturn_L % 360
    mars_M = mars_L % 360
    
    # Calculate heliocentric distances
    earth_r = mean_anomaly_to_distance(earth_a, earth_e, earth_M)
    jupiter_r = mean_anomaly_to_distance(jupiter_a, jupiter_e, jupiter_M)
    saturn_r = mean_anomaly_to_distance(saturn_a, saturn_e, saturn_M)
    mars_r = mean_anomaly_to_distance(mars_a, mars_e, mars_M)
    
    # Calculate Earth-Planet distances (simplified - assumes coplanar orbits)
    jupiter_angle = math.radians(abs(earth_L - jupiter_L))
    saturn_angle = math.radians(abs(earth_L - saturn_L))
    mars_angle = math.radians(abs(earth_L - mars_L))
    
    earth_jupiter_dist = math.sqrt(earth_r**2 + jupiter_r**2 - 2*earth_r*jupiter_r*math.cos(jupiter_angle))
    earth_saturn_dist = math.sqrt(earth_r**2 + saturn_r**2 - 2*earth_r*saturn_r*math.cos(saturn_angle))
    earth_mars_dist = math.sqrt(earth_r**2 + mars_r**2 - 2*earth_r*mars_r*math.cos(mars_angle))
    
    # Calculate Earth orbital velocity (varies with distance)
    earth_velocity = 29.78 * math.sqrt(2/earth_r - 1/earth_a)  # km/s
    
    return {
        'date': date.strftime('%Y-%m-%d'),
        'earth_heliocentric_distance': earth_r,
        'earth_orbital_velocity': earth_velocity,
        'jupiter_distance': earth_jupiter_dist,
        'saturn_distance': earth_saturn_dist,
        'mars_distance': earth_mars_dist,
        'jupiter_elongation': abs(earth_L - jupiter_L) % 360,
        'saturn_elongation': abs(earth_L - saturn_L) % 360,
        'mars_elongation': abs(earth_L - mars_L) % 360
    }

def _prewhiten_ar1(series: np.ndarray) -> Tuple[np.ndarray, float]:
    """Fit AR(1) and return residuals and phi."""
    y = np.asarray(series, dtype=float)
    y = y - np.nanmean(y)
    # Guard against degenerate variance
    if y.size < 3 or np.nanstd(y) == 0:
        return y - np.nanmean(y), 0.0
    y0 = y[:-1]
    y1 = y[1:]
    # Handle NaNs
    mask = np.isfinite(y0) & np.isfinite(y1)
    if mask.sum() < 3:
        return y - np.nanmean(y), 0.0
    phi = float(np.corrcoef(y0[mask], y1[mask])[0, 1]) if np.nanstd(y0[mask]) > 0 and np.nanstd(y1[mask]) > 0 else 0.0
    phi = 0.0 if not np.isfinite(phi) else max(min(phi, 0.99), -0.99)
    resid = np.empty_like(y)
    resid[0] = 0.0
    resid[1:] = y[1:] - phi * y[:-1]
    return resid - np.nanmean(resid), phi

def _remove_known_harmonics(series: np.ndarray, periods_days: List[float]) -> np.ndarray:
    """Project out known sinusoidal components via least squares regression."""
    y = np.asarray(series, dtype=float)
    n = y.size
    t = np.arange(n, dtype=float)
    # Design matrix: intercept + trend
    X_cols = [np.ones(n), t]
    for P in periods_days:
        if P <= 0:
            continue
        w = 2.0 * math.pi / P
        X_cols.append(np.sin(w * t))
        X_cols.append(np.cos(w * t))
    X = np.vstack(X_cols).T
    # Least squares fit
    try:
        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)
        y_fit = X @ beta
        resid = y - y_fit
        return resid - np.nanmean(resid)
    except Exception:
        return y - np.nanmean(y)

def _multitaper_psd(series: np.ndarray, fs: float = 1.0, NW: float = 2.5, Kmax: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
    """Compute multitaper power spectral density using DPSS tapers.
    Returns (frequencies[cycles/day], psd).
    """
    from scipy.signal.windows import dpss
    y = np.asarray(series, dtype=float)
    n = y.size
    y = y - np.nanmean(y)
    if Kmax is None:
        # Common choice: K = floor(2*NW - 1)
        Kmax = max(1, int(math.floor(2 * NW - 1)))
    tapers, eigs = dpss(n, NW, Kmax, return_ratios=True)
    # rfft frequencies in cycles/day
    freqs = np.fft.rfftfreq(n, d=1.0 / fs)
    psd_accum = np.zeros_like(freqs, dtype=float)
    for k in range(tapers.shape[0]):
        tapered = y * tapers[k]
        spec = np.fft.rfft(tapered)
        # Standard periodogram scaling (per sample)
        pxx = (np.abs(spec) ** 2) / (n * fs)
        psd_accum += pxx
    psd = psd_accum / tapers.shape[0]
    return freqs, psd

def _local_snr(psd: np.ndarray, idx: int, window: int = 8) -> Tuple[float, float]:
    """Compute SNR as peak power over local median noise (excluding center bin)."""
    n = psd.size
    lo = max(0, idx - window)
    hi = min(n, idx + window + 1)
    neigh = np.concatenate([psd[lo:idx], psd[idx + 1:hi]]) if hi > lo else np.array([])
    noise = float(np.median(neigh)) if neigh.size else 0.0
    peak = float(psd[idx])
    snr = peak / noise if noise > 0 else 0.0
    return snr, noise

def _block_bootstrap_pvals(series: np.ndarray, fs: float, target_freqs: List[float], idxs: List[int],
                           n_iter: int = 200, block_size: int = 28, NW: float = 2.5, Kmax: Optional[int] = None,
                           seed: int = 42) -> Dict[int, float]:
    """Empirical p-values for powers at target indices using moving block bootstrap."""
    rng = np.random.default_rng(seed)
    y = np.asarray(series, dtype=float)
    n = y.size
    if n < 10:
        return {i: 1.0 for i in idxs}
    # Precompute observed PSD
    freqs_obs, psd_obs = _multitaper_psd(y, fs=fs, NW=NW, Kmax=Kmax)
    obs_power = {i: float(psd_obs[i]) for i in idxs}
    # Prepare blocks
    n_blocks = max(1, int(math.ceil(n / block_size)))
    starts = np.arange(0, n - block_size + 1)
    exceed_counts = {i: 0 for i in idxs}
    for _ in range(n_iter):
        if starts.size == 0:
            yb = y.copy()
        else:
            picks = rng.choice(starts, size=n_blocks, replace=True)
            segs = [y[s:s + block_size] for s in picks]
            yb = np.concatenate(segs)[:n]
        yb = yb - np.nanmean(yb)
        freqs_b, psd_b = _multitaper_psd(yb, fs=fs, NW=NW, Kmax=Kmax)
        for i in idxs:
            if i < len(psd_b):
                if float(psd_b[i]) >= obs_power[i]:
                    exceed_counts[i] += 1
    pvals = {i: (exceed_counts[i] + 1) / (n_iter + 1) for i in idxs}
    return pvals

def _cone_of_influence(n: int, dt: float = 1.0, wavelet: str = 'cmor1.5-1.0') -> np.ndarray:
    """Calculate cone of influence for wavelet analysis."""
    import pywt
    # For Morlet wavelets, COI is approximately sqrt(2) * scale
    fourier_factor = pywt.central_frequency(wavelet, precision=10)
    coi_factor = np.sqrt(2)  # For Morlet wavelets
    
    # Time vector
    t = np.arange(n) * dt
    # COI extends from edges
    coi = np.minimum(t, n*dt - t) / coi_factor
    return coi

def _ar1_spectrum(n: int, alpha: float, dt: float = 1.0) -> np.ndarray:
    """Generate theoretical AR(1) red noise spectrum."""
    freqs = np.fft.fftfreq(n, dt)
    freqs = freqs[:n//2 + 1]  # Positive frequencies only
    
    # AR(1) theoretical spectrum: S(f) = (1-alpha^2) / (1 + alpha^2 - 2*alpha*cos(2*pi*f))
    spectrum = (1 - alpha**2) / (1 + alpha**2 - 2*alpha*np.cos(2*np.pi*freqs*dt))
    return freqs, spectrum

def _generate_ar1_surrogates(data: np.ndarray, alpha: float, n_surrogates: int = 1000) -> np.ndarray:
    """Generate AR(1) surrogate time series for significance testing."""
    n = len(data)
    surrogates = np.zeros((n_surrogates, n))
    
    # Generate AR(1) surrogates with same variance as data
    data_var = np.var(data)
    noise_var = data_var * (1 - alpha**2)
    
    for i in range(n_surrogates):
        surrogate = np.zeros(n)
        surrogate[0] = np.random.normal(0, np.sqrt(data_var))
        
        for t in range(1, n):
            surrogate[t] = alpha * surrogate[t-1] + np.random.normal(0, np.sqrt(noise_var))
            
        surrogates[i] = surrogate
    
    return surrogates

def _extract_band_power(coeffs: np.ndarray, periods: np.ndarray, target_period: float, bandwidth: float = 0.2) -> np.ndarray:
    """Extract band power time series around target period."""
    # Find period indices within bandwidth
    period_mask = np.abs(periods - target_period) <= (target_period * bandwidth)
    
    if not np.any(period_mask):
        return np.zeros(coeffs.shape[1])
    
    # Average power across the band
    band_power = np.mean(np.abs(coeffs[period_mask, :])**2, axis=0)  # Power is squared magnitude
    return band_power

def _find_stable_peak_period(power_slice: np.ndarray, periods: np.ndarray, window: int = 3) -> float:
    """Find a stable peak period using a weighted average around the max power."""
    if power_slice.size == 0:
        return np.nan
        
    peak_idx = np.argmax(power_slice)
    
    # Define a window around the peak
    start = max(0, peak_idx - window)
    end = min(len(power_slice), peak_idx + window + 1)
    
    window_periods = periods[start:end]
    window_power = power_slice[start:end]
    
    # Weighted average of period, weighted by power
    total_power = np.sum(window_power)
    if total_power == 0:
        return periods[peak_idx] # Fallback to simple max
        
    stable_period = np.sum(window_periods * window_power) / total_power
    return stable_period

def analyze_hilbert_instantaneous_frequency(analysis_center: str = 'merged') -> Dict:
    """
    Option A: Bandpass + Hilbert instantaneous frequency analysis with amplitude gating
    and event-locked permutation tests for selected bands.
    """
    try:
        print_status("Starting Hilbert Instantaneous Frequency Analysis...", "PROCESS")
        import numpy as np
        import pandas as pd
        from scipy.signal import firwin, filtfilt, hilbert
        
        # Load daily coherence
        df = load_geospatial_data(analysis_center)
        daily = df.groupby(df['date'].dt.date)['coherence'].mean()
        dates = sorted(daily.index)
        y = np.array([daily[d] for d in dates], dtype=float)
        n = len(y)
        if n < 200:
            return {'success': False, 'error': 'Insufficient data length for IF analysis'}
        
        # Bands to analyze (periods in days)
        bands = {
            'solar_rotation_27d': 27.27,
            'lunar_month_29d': 29.53,
            'dominant_112d': 112.0,
            'js_beat_19d': 19.86,
        }
        
        fs = 1.0  # samples per day
        results_bands = {}
        
        def bandpass(signal, center_period, rel_bw=0.25):
            center_f = 1.0 / center_period
            bw = center_f * rel_bw
            f1 = max(1e-4, center_f - bw)
            f2 = min(0.49, center_f + bw)
            # Taps: longer for long periods, but safe for filtfilt padlen
            proposed = int(max(101, 6 * center_period))
            # Ensure odd number
            proposed = proposed if proposed % 2 == 1 else proposed + 1
            max_taps = max(31, ((n - 1) // 3) - 1)
            numtaps = int(min(proposed, max_taps))
            numtaps = numtaps if numtaps % 2 == 1 else numtaps - 1
            taps = firwin(numtaps, [f1, f2], pass_zero=False, fs=fs)
            return filtfilt(taps, [1.0], signal, padlen=min(3*len(taps), n-2))
        
        # Event list for tests
        events = [
            ('2023-11-03', 'Jupiter Opposition'),
            ('2024-09-08', 'Saturn Opposition'),
            ('2024-12-07', 'Jupiter Opposition'),
            ('2025-01-16', 'Mars Opposition'),
            ('2024-04-08', 'Solar Eclipse'),
            ('2024-01-03', 'Perihelion'),
            ('2024-07-05', 'Aphelion'),
        ]
        event_idxs = []
        date_series = pd.Series(np.arange(n), index=pd.to_datetime(dates))
        for d, _ in events:
            ts = pd.to_datetime(d)
            if ts in date_series.index:
                event_idxs.append(int(date_series.loc[ts]))
        
        rng = np.random.default_rng(42)
        n_perm = 2000
        
        for name, period in bands.items():
            # Narrower bandwidth for 112d band to avoid leakage
            rel_bw = 0.25
            if '112' in name:
                rel_bw = 0.15
            x = bandpass(y, period, rel_bw=rel_bw)
            analytic = hilbert(x)
            amp = np.abs(analytic)
            phase = np.unwrap(np.angle(analytic))
            inst_freq = np.gradient(phase) / (2*np.pi)  # cycles/day
            # Invalidate pathological instantaneous frequencies
            inst_freq[np.isnan(inst_freq)] = 0.0
            inst_period = np.full_like(inst_freq, np.nan)
            small = np.abs(inst_freq) < (1.0 / (10.0 * period))
            valid = ~small & (inst_freq > 0)
            inst_period[valid] = 1.0 / inst_freq[valid]
            
            # Amplitude gating: keep only high-SNR regions
            thr_percentile = 60 if period < 60 else 70
            thr = np.percentile(amp, thr_percentile)
            mask = amp >= thr
            inst_period_gated = np.where(mask, inst_period, np.nan)
            # Smooth the gated series for plotting/robustness
            from scipy.ndimage import gaussian_filter1d
            inst_period_smooth = gaussian_filter1d(np.where(np.isnan(inst_period_gated), 0, inst_period_gated), sigma=2.0)
            inst_period_smooth[np.isnan(inst_period_gated)] = np.nan
            
            # Event-locked test: window Â±15 days
            win = 15
            effects = []
            for idx in event_idxs:
                s = max(0, idx - win)
                e = min(n, idx + win + 1)
                wvals = inst_period_gated[s:e]
                bvals = np.concatenate([inst_period_gated[max(0, s-60):s], inst_period_gated[e:min(n, e+60)]])
                wvals = wvals[~np.isnan(wvals)]
                bvals = bvals[~np.isnan(bvals)]
                if len(wvals) >= 5 and len(bvals) >= 20:
                    effects.append(np.nanstd(wvals) - np.nanstd(bvals))
            observed_effect = float(np.nanmean(effects)) if effects else np.nan
            
            # Permutation: circularly shift series to preserve autocorrelation
            perm_effects = []
            if not np.isnan(observed_effect):
                for _ in range(n_perm):
                    shift = rng.integers(0, n)
                    inst_shift = np.roll(inst_period_gated, shift)
                    peffects = []
                    for idx in event_idxs:
                        s = max(0, idx - win)
                        e = min(n, idx + win + 1)
                        wvals = inst_shift[s:e]
                        bvals = np.concatenate([inst_shift[max(0, s-60):s], inst_shift[e:min(n, e+60)]])
                        wvals = wvals[~np.isnan(wvals)]
                        bvals = bvals[~np.isnan(bvals)]
                        if len(wvals) >= 5 and len(bvals) >= 20:
                            peffects.append(np.nanstd(wvals) - np.nanstd(bvals))
                    if peffects:
                        perm_effects.append(np.nanmean(peffects))
            pval = float(np.mean(np.array(perm_effects) >= observed_effect)) if perm_effects else 1.0
            
            results_bands[name] = {
                'target_period_days': period,
                'mean_instantaneous_period_days': float(np.nanmean(inst_period_smooth)),
                'std_instantaneous_period_days': float(np.nanstd(inst_period_smooth)),
                'amplitude_threshold': float(thr),
                'event_locked_effect': observed_effect,
                'permutation_p_value': pval,
                'rel_bandwidth': rel_bw
            }
        
        # Save improved plot showing instantaneous period over time
        import matplotlib.pyplot as plt
        fig, axes = plt.subplots(len(bands), 1, figsize=(14, 2.6*len(bands)), sharex=True)
        if len(bands) == 1:
            axes = [axes]
        # Prepare event markers
        event_lines = []
        for d, _ in events:
            ts = pd.to_datetime(d)
            if ts in date_series.index:
                event_lines.append(int(date_series.loc[ts]))
        for ax, (name, period) in zip(axes, bands.items()):
            # Recompute series for plotting from stored stats if needed
            rel_bw = results_bands[name]['rel_bandwidth']
            x = bandpass(y, period, rel_bw=rel_bw)
            analytic = hilbert(x)
            amp = np.abs(analytic)
            phase = np.unwrap(np.angle(analytic))
            inst_freq = np.gradient(phase) / (2*np.pi)
            inst_period = np.full_like(inst_freq, np.nan)
            small = np.abs(inst_freq) < (1.0 / (10.0 * period))
            valid = ~small & (inst_freq > 0)
            inst_period[valid] = 1.0 / inst_freq[valid]
            thr_percentile = 60 if period < 60 else 70
            thr = np.percentile(amp, thr_percentile)
            mask = amp >= thr
            series = np.where(mask, inst_period, np.nan)
            from scipy.ndimage import gaussian_filter1d as gf
            series_s = gf(np.where(np.isnan(series), 0, series), sigma=2.0)
            series_s[np.isnan(series)] = np.nan
            ax.plot(series_s, color='tab:blue', linewidth=1.5)
            ax.axhline(y=period, color='black', linestyle='--', alpha=0.6, linewidth=1)
            for evx in event_lines:
                ax.axvline(x=evx, color='red', alpha=0.2, linewidth=1)
            ax.set_ylabel(f"{name}\n(days)")
            # Set sensible y-limits around target
            if period < 60:
                ax.set_ylim(period*0.8, period*1.2)
            elif period <= 140:
                ax.set_ylim(max(60, period*0.7), period*1.3)
            else:
                ax.set_ylim(max(60, period*0.7), period*1.3)
            ax.grid(alpha=0.3)
        axes[-1].set_xlabel('Days index')
        plot_path = ROOT / 'results' / 'figures' / f'step_10_hilbert_if_{analysis_center}.png'
        plt.tight_layout()
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print_status(f"Hilbert IF plot saved: {plot_path}", "SUCCESS")
        
        return {
            'success': True,
            'analysis_center': analysis_center,
            'analysis_type': 'hilbert_instantaneous_frequency',
            'bands': results_bands,
            'plot_path': str(plot_path)
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_wavelet_time_frequency(analysis_center: str = 'merged', verbose: bool = True) -> Dict:
    """
    Restored wavelet analysis for discovering the ~112-day signal.
    """
    try:
        print_status("Starting Wavelet Time-Frequency Analysis...", "PROCESS")

        # Load and preprocess data
        df = load_geospatial_data(analysis_center)
        daily_coherence = df.groupby(df['date'].dt.date)['coherence'].mean()
        unique_dates = sorted(daily_coherence.index)

        y = np.array([daily_coherence[date] for date in unique_dates], dtype=float)

        # Remove dominant Earth signals but keep orbital signals
        known_periods = [365.2422, 182.6211, 29.53059, 27.21222, 14.765]
        y_clean = _remove_known_harmonics(y, known_periods)
        y_white, ar1_coeff = _prewhiten_ar1(y_clean)

        # Wavelet analysis
        import pywt
        wavelet = 'cmor1.5-1.0'
        min_period, max_period = 20, 200  # Days - focus on orbital periods
        n_scales = 50

        fourier_factor = pywt.central_frequency(wavelet, precision=10)
        scales = (fourier_factor * 1.0) / (np.linspace(1./max_period, 1./min_period, n_scales))

        coeffs, freqs = pywt.cwt(y_white, scales, wavelet, sampling_period=1.0)
        periods = 1.0 / freqs
        power = np.abs(coeffs)**2  # Power is squared magnitude, not just magnitude

        # Find dominant ~112d signal
        target_112d = 112.0
        period_idx_112 = np.argmin(np.abs(periods - target_112d))
        signal_112d = power[period_idx_112, :]
        max_power_112d = np.max(signal_112d)

        # Create styled plot with site theme
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates
        import matplotlib as mpl
        
        # Set site theme styling
        mpl.rcParams.update({
            'font.family': 'Times New Roman',
            'font.size': 12,
            'axes.titlesize': 14,
            'axes.labelsize': 12,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10,
            'legend.fontsize': 10,
            'figure.titlesize': 16,
            'axes.grid': True,
            'grid.color': '#495773',
            'grid.linestyle': '--',
            'grid.linewidth': 0.5,
            'axes.edgecolor': '#1e4a5f',
            'axes.labelcolor': '#1e4a5f',
            'axes.titlecolor': '#2D0140',
            'xtick.color': '#1e4a5f',
            'ytick.color': '#1e4a5f',
            'text.color': '#1e4a5f',
        })

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), facecolor='white')

        # Scalogram with site theme colors and proper y-axis
        extent = [0, len(y_white), periods.min(), periods.max()]
        im = ax1.imshow(power, aspect='auto', extent=extent, origin='lower', cmap='viridis')
        ax1.axhline(y=target_112d, color='#F2EC99', linestyle='--', linewidth=3, 
                   label='112d signal', alpha=0.9)
        ax1.set_title(f'Wavelet Scalogram - {analysis_center.upper()}', 
                     fontsize=14, color='#2D0140', fontweight='bold')
        ax1.set_xlabel('Days from Start', fontsize=12, color='#1e4a5f')
        ax1.set_ylabel('Period [days]', fontsize=12, color='#1e4a5f')
        
        # Set meaningful y-axis ticks for periods
        period_ticks = [20, 30, 50, 80, 112, 150, 200]
        period_ticks = [p for p in period_ticks if periods.min() <= p <= periods.max()]
        ax1.set_yticks(period_ticks)
        ax1.set_yticklabels([f'{p}' for p in period_ticks])
        
        # Style colorbar
        cbar = plt.colorbar(im, ax=ax1, label='Power')
        cbar.set_label('Power', color='#1e4a5f', fontsize=12)
        cbar.ax.tick_params(colors='#1e4a5f')
        
        # Style axes
        ax1.tick_params(colors='#1e4a5f')
        ax1.spines['top'].set_color('#1e4a5f')
        ax1.spines['bottom'].set_color('#1e4a5f')
        ax1.spines['left'].set_color('#1e4a5f')
        ax1.spines['right'].set_color('#1e4a5f')

        # 112d signal evolution with site theme
        ax2.plot(signal_112d, color='#2D0140', linewidth=2.5, alpha=0.8)
        ax2.set_title('112-Day Signal Evolution', fontsize=14, color='#2D0140', fontweight='bold')
        ax2.set_xlabel('Days from Start', fontsize=12, color='#1e4a5f')
        ax2.set_ylabel('Power', fontsize=12, color='#1e4a5f')
        ax2.grid(True, alpha=0.3, color='#495773', linestyle='--')
        
        # Style second axes
        ax2.tick_params(colors='#1e4a5f')
        ax2.spines['top'].set_color('#1e4a5f')
        ax2.spines['bottom'].set_color('#1e4a5f')
        ax2.spines['left'].set_color('#1e4a5f')
        ax2.spines['right'].set_color('#1e4a5f')

        plt.tight_layout()

        # Save plot
        plot_filename = f"step_10_wavelet_restored_{analysis_center}.png"
        plot_path = ROOT / "results" / "figures" / plot_filename
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        print_status(f"Wavelet analysis plot saved: {plot_path}", "SUCCESS")

        results = {
            'success': True,
            'analysis_center': analysis_center,
            'analysis_type': 'wavelet_time_frequency_restored',
            'plot_path': str(plot_path),
            'dominant_112d_signal': {
                'target_period': target_112d,
                'max_power': float(max_power_112d),
                'signal_evolution': signal_112d.tolist()
            },
            'ar1_coefficient': float(ar1_coeff),
            'interpretation': f'Restored wavelet analysis for {analysis_center.upper()}. Detected ~112d signal with max power {max_power_112d:.4f}.'
        }

        return results

    except ImportError as e:
        return {'success': False, 'error': f"Missing PyWavelets: {e}"}
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_comprehensive_step_10(analysis_center: str = 'merged', resolution: str = '1min') -> Dict:
    """
    Run all Step 10 analyses: eclipses, planetary oppositions, wavelet, and Hilbert-IF.
    This is the default comprehensive analysis.
    """
    try:
        print_status("Starting COMPREHENSIVE Step 10 Analysis...", "PROCESS")
        print_status("Running all eclipses, planetary oppositions, wavelet, and frequency analyses", "INFO")
        
        comprehensive_results = {
            'success': True,
            'analysis_center': analysis_center,
            'analysis_type': 'comprehensive_step_10',
            'analyses_completed': {}
        }
        
        # 1. All Eclipses Analysis
        print_status("=== ECLIPSE ANALYSIS ===", "HEADER")
        eclipse_results = analyze_all_eclipses_comprehensive(analysis_center, resolution)
        comprehensive_results['analyses_completed']['eclipses'] = eclipse_results
        
        # 2. All Astronomical Events (Jupiter, Saturn, Mars, Lunar)
        print_status("=== PLANETARY OPPOSITION ANALYSIS ===", "HEADER")
        astronomical_results = analyze_all_astronomical_events(analysis_center)
        comprehensive_results['analyses_completed']['astronomical_events'] = astronomical_results
        
        # 3. Wavelet Analysis (~112d signal)
        print_status("=== WAVELET TIME-FREQUENCY ANALYSIS ===", "HEADER")
        wavelet_results = analyze_wavelet_time_frequency(analysis_center)
        comprehensive_results['analyses_completed']['wavelet'] = wavelet_results
        
        # 4. Hilbert Instantaneous Frequency Analysis
        print_status("=== HILBERT INSTANTANEOUS FREQUENCY ANALYSIS ===", "HEADER")
        hilbert_results = analyze_hilbert_instantaneous_frequency(analysis_center)
        comprehensive_results['analyses_completed']['hilbert_if'] = hilbert_results
        
        # Summary
        successful_analyses = sum(1 for result in comprehensive_results['analyses_completed'].values() 
                                if result.get('success', False))
        total_analyses = len(comprehensive_results['analyses_completed'])
        
        print_status("=== COMPREHENSIVE ANALYSIS SUMMARY ===", "HEADER")
        print_status(f"Completed {successful_analyses}/{total_analyses} analyses successfully", "SUCCESS")
        
        # Extract key findings
        key_findings = []
        
        # Eclipse findings
        if eclipse_results.get('success'):
            n_eclipses = len(eclipse_results.get('eclipses_analyzed', []))
            key_findings.append(f"Eclipse Type Hierarchy: {n_eclipses} eclipses analyzed")
        
        # Astronomical findings  
        if astronomical_results.get('success'):
            events = astronomical_results.get('events_analyzed', {})
            successful_events = sum(1 for event in events.values() if event.get('success', False))
            key_findings.append(f"Planetary Oppositions: {successful_events} events analyzed")
        
        # Wavelet findings
        if wavelet_results.get('success'):
            max_power = wavelet_results.get('dominant_112d_signal', {}).get('max_power', 0)
            key_findings.append(f"~112d Signal: max power {max_power:.4f}")
        
        # Hilbert findings
        if hilbert_results.get('success'):
            bands = hilbert_results.get('bands', {})
            significant_bands = [name for name, data in bands.items() 
                               if data.get('permutation_p_value', 1.0) < 0.1]
            if significant_bands:
                key_findings.append(f"Frequency Modulation: {len(significant_bands)} significant bands")
        
        comprehensive_results['key_findings'] = key_findings
        comprehensive_results['interpretation'] = f"Comprehensive Step 10 analysis complete. {successful_analyses}/{total_analyses} analyses successful. Key findings: {'; '.join(key_findings)}"
        
        return comprehensive_results
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def _get_barcode_on_off_series(analysis_center: str) -> np.ndarray:
    """Helper function to generate the binary on/off time series for the barcode region."""
    df = load_geospatial_data(analysis_center)
    daily_coherence = df.groupby(df['date'].dt.date)['coherence'].mean()
    y = np.array([daily_coherence[date] for date in sorted(daily_coherence.index)], dtype=float)
    known_periods = [365.2422, 182.6211, 29.53059, 27.21222, 14.765, 433.0]
    y_clean = _remove_known_harmonics(y, known_periods)
    y_white, phi_ar1 = _prewhiten_ar1(y_clean)

    import pywt
    wavelet = 'cmor1.5-1.0'
    min_period, max_period, total_scales = 15, 80, 50 # Barcode region
    fourier_factor = pywt.central_frequency(wavelet, precision=10)
    scales = (fourier_factor * 1.0) / (np.linspace(1./max_period, 1./min_period, total_scales))
    
    coeffs, freqs = pywt.cwt(y_white, scales, wavelet, sampling_period=1.0)
    power = np.abs(coeffs)**2  # Power is squared magnitude

    # Generate surrogates to find significance threshold
    surrogates = _generate_ar1_surrogates(y_white, phi_ar1, n_surrogates=100)
    surrogate_powers = []
    for surrogate in surrogates:
        coeffs_surr, _ = pywt.cwt(surrogate, scales, wavelet, sampling_period=1.0)
        surrogate_powers.append(np.mean(np.abs(coeffs_surr)))
    
    significance_threshold = np.percentile(surrogate_powers, 95)

    mean_power_per_day = np.mean(power, axis=0)
    on_off_series = (mean_power_per_day > significance_threshold).astype(int)
    return on_off_series

def main():
    """Main function for Step 10 high-resolution astronomical event analysis."""
    import argparse
    
    parser = argparse.ArgumentParser(description="TEP GNSS High-Resolution Astronomical Events - Step 10")
    parser.add_argument('--event', choices=['eclipse', 'all-eclipses', 'differential', 'jupiter', 'saturn', 'mars', 'mercury', 'venus', 'gravitational-field', 'supermoon', 'lunar', 'all-astronomical', 'wavelet-analysis', 'hilbert-if', 'comprehensive', 'advanced-sham-controls', 'orbital-periodicity'], default='comprehensive',
                        help='Event type to analyze at high resolution')
    parser.add_argument('--center', choices=['code', 'igs_combined', 'esa_final', 'merged'], default='merged',
                        help='GPS analysis center to process, or "merged" to combine all three')
    parser.add_argument('--resolution', choices=['30s', '1min', '5min', '15min', '30min'], default='1min',
                        help='Temporal resolution for analysis')
    parser.add_argument('--eclipse-date', default='all',
                        help='Eclipse date in YYYY-MM-DD format or "all" for all eclipses (default: all)')
    parser.add_argument('--quiet', action='store_true',
                        help='Suppress detailed output, show only summary results')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("TEP GNSS Analysis Package v0.7")
    print("STEP 10: High-Resolution Astronomical Event Analysis")
    print("=" * 80)
    print(f"Event: {args.event.upper()}")
    print(f"Center: {args.center.upper()}")
    print(f"Resolution: {args.resolution}")
    if args.event == 'differential':
        print(f"Eclipse Date: {args.eclipse_date}")
    print("=" * 80)
    
    start_time = time.time()
    
    # Core astronomical event analysis (streamlined)
    if args.event == 'comprehensive':
        # Default: Run everything (eclipses, planets, wavelet, hilbert) for all centers
        if args.center == 'merged':
            # Run for all centers when merged is specified
            all_centers = ['code', 'igs_combined', 'esa_final', 'merged']
            results = {}
            for center in all_centers:
                print(f"\n{'='*80}")
                print(f"COMPREHENSIVE ANALYSIS FOR CENTER: {center.upper()}")
                print(f"{'='*80}")
                center_results = analyze_comprehensive_step_10(center, args.resolution)
                results[center] = center_results
            
            # Create summary results
            results = {
                'success': True,
                'analysis_type': 'comprehensive_multi_center',
                'centers_analyzed': all_centers,
                'center_results': results,
                'interpretation': f'Comprehensive analysis completed for {len(all_centers)} analysis centers'
            }
        else:
            # Single center analysis
            results = analyze_comprehensive_step_10(args.center, args.resolution)
    elif args.event == 'advanced-sham-controls':
        results = analyze_advanced_sham_controls(args.center)
    elif args.event == 'orbital-periodicity':
        results = analyze_orbital_periodicity_effects(args.center)
    elif args.event == 'eclipse':
        results = analyze_solar_eclipse_high_resolution(args.center)
    elif args.event == 'all-eclipses':
        results = analyze_all_eclipses_comprehensive(args.center, args.resolution)
    elif args.event == 'differential':
        results = analyze_eclipse_differential_coherence(args.center, args.resolution, args.eclipse_date)
    elif args.event == 'jupiter':
        results = analyze_jupiter_opposition_high_resolution_DEPRECATED(args.center)
    elif args.event == 'saturn':
        results = analyze_saturn_opposition_high_resolution_DEPRECATED(args.center)
    elif args.event == 'mars':
        results = analyze_mars_opposition_high_resolution_DEPRECATED(args.center)
    elif args.event == 'mercury':
        results = analyze_mercury_opposition_high_resolution_DEPRECATED(args.center)
    elif args.event == 'venus':
        results = analyze_venus_elongation_high_resolution_DEPRECATED(args.center)
    elif args.event == 'gravitational-field':
        results = analyze_continuous_gravitational_field(args.center)
    elif args.event == 'supermoon':
        results = analyze_supermoon_perigee_high_resolution(args.center)
    elif args.event == 'lunar':
        results = analyze_lunar_standstill_high_resolution(args.center)
    elif args.event == 'all-astronomical':
        results = analyze_all_astronomical_events(args.center)
    elif args.event == 'wavelet-analysis':
        # Restored wavelet analysis for ~112d signal discovery
        results = analyze_wavelet_time_frequency(args.center)
    elif args.event == 'hilbert-if':
        # Basic instantaneous frequency analysis for IGS 27d signal
        results = analyze_hilbert_instantaneous_frequency(args.center)
    else:
        print_status(f"Event type '{args.event}' not yet implemented", "ERROR")
        return False
    
    # Print results
    if results.get('success'):
        print("\n" + "=" * 60)
        print(f"HIGH-RESOLUTION {args.event.upper()} ANALYSIS RESULTS")
        print("=" * 60)
        
        # Only print analysis center if it exists (validate-barcode doesn't have one)
        if 'analysis_center' in results:
            print(f"Analysis Center: {results['analysis_center'].upper()}")
        
        if 'temporal_resolution_minutes' in results:
            print(f"Temporal Resolution: {results['temporal_resolution_minutes']} minutes")
        
        if 'interpretation' in results:
            print(f"Interpretation: {results['interpretation']}")
        
        # Handle validate-barcode specific output
        if args.event == 'validate-barcode' and 'summary_table' in results:
            print("\nValidation completed successfully. Summary table was displayed above.")
        
        # Handle different result types
        if args.event == 'all-eclipses':
            # Eclipse comprehensive results
            if 'eclipses_analyzed' in results:
                print(f"\nECLIPSES ANALYZED: {len(results['eclipses_analyzed'])}")
                print("-" * 45)
                for eclipse in results['eclipses_analyzed']:
                    print(f"{eclipse['date']} ({eclipse['type']}): {eclipse['effect_percent']:+.1f}%")
                
                if 'eclipse_summary' in results:
                    print(f"\nECLIPSE TYPE SUMMARY:")
                    print("-" * 25)
                    for eclipse_type, summary in results['eclipse_summary'].items():
                        avg_effect = summary['average_effect_percent']
                        n_events = summary['number_of_events']
                        print(f"{eclipse_type:<10}: {avg_effect:+.1f}% (n={n_events})")
        
        elif args.event in ['jupiter', 'saturn', 'mars', 'supermoon']:
            # Opposition/Perigee results
            if args.event == 'supermoon':
                opposition_key = 'supermoon_perigees'
            else:
                opposition_key = f"{args.event}_oppositions"
            if opposition_key in results and results[opposition_key]:
                if args.event == 'supermoon':
                    print(f"\nSUPERMOON PERIGEES:")
                    print("-" * 30)
                    for perigee in results[opposition_key]:
                        print(f"{perigee['date']}: {perigee['effect_size_percent']:+.3f}% effect")
                    # Show stacked statistics for supermoon
                    if 'average_effect_percent' in results:
                        avg = results['average_effect_percent']
                        sem = results.get('standard_error', 0)
                        n = results.get('n_events', 0)
                        sig_status = "SIGNIFICANT" if results.get('statistically_significant') else "not significant"
                        print(f"\nStacked Analysis: {avg:+.3f}Â±{sem:.3f}% across {n} events ({sig_status})")
                        print(f"Detection Status: {results.get('detection_status', 'Unknown')}")
                else:
                    print(f"\n{args.event.upper()} OPPOSITIONS:")
                    print("-" * 30)
                    for opposition in results[opposition_key]:
                        print(f"{opposition['date']}: {opposition['effect_size_percent']:+.2f}% effect")
                
                if 'average_effect_percent' in results:
                    print(f"\nAverage Effect: {results['average_effect_percent']:+.2f}%")
        
        elif args.event == 'lunar':
            # Lunar standstill results
            if 'enhancement_percent' in results:
                print(f"\nLUNAR STANDSTILL ENHANCEMENT:")
                print("-" * 35)
                print(f"Pre-standstill variability: {results.get('pre_standstill_variability', 0):.6f}")
                print(f"Standstill variability: {results.get('standstill_variability', 0):.6f}")
                print(f"Enhancement: {results['enhancement_percent']:+.2f}%")
        
        elif args.event == 'all-astronomical':
            # Comprehensive astronomical results
            if 'events_analyzed' in results:
                print(f"\nCOMPREHENSIVE ASTRONOMICAL ANALYSIS:")
                print("-" * 40)
                for event_type, event_results in results['events_analyzed'].items():
                    if event_results.get('success'):
                        interpretation = event_results.get('interpretation', 'Analysis completed')
                        print(f"{event_type.upper():<10}: {interpretation}")
        
        elif args.event == 'daily-orbital':
            # Daily orbital tracking results
            if 'correlations' in results:
                print(f"\nDAILY ORBITAL CORRELATIONS:")
                print("-" * 35)
                correlations = results['correlations']
                p_values = results.get('p_values', {})
                
                for planet in ['jupiter', 'saturn', 'mars']:
                    corr_key = f'{planet}_distance_correlation'
                    p_key = f'{planet}_distance_p_value'
                    if corr_key in correlations:
                        corr = correlations[corr_key]
                        p_val = p_values.get(p_key, 1.0)
                        significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else ""
                        print(f"{planet.capitalize():<8}: r = {corr:+.3f} (p = {p_val:.4f}){significance}")
                
                # Earth velocity correlation
                if 'earth_velocity_correlation' in correlations:
                    vel_corr = correlations['earth_velocity_correlation']
                    vel_p = p_values.get('earth_velocity_p_value', 1.0)
                    significance = "***" if vel_p < 0.001 else "**" if vel_p < 0.01 else "*" if vel_p < 0.05 else ""
                    print(f"{'Earth Vel':<8}: r = {vel_corr:+.3f} (p = {vel_p:.4f}){significance}")
                
                print(f"\nStrongest correlation: {results.get('strongest_planetary_correlation', 'Unknown')}")
                print(f"Days analyzed: {results.get('days_analyzed', 0)}")
        
        elif args.event == 'synodic-cycles':
            # Synodic cycle results
            if 'synodic_cycles' in results:
                print(f"\nSYNODIC CYCLE ANALYSIS:")
                print("-" * 30)
                for planet, cycle_data in results['synodic_cycles'].items():
                    enhancement = cycle_data['opposition_enhancement_percent']
                    cycles = cycle_data['cycles_in_data']
                    period = cycle_data['synodic_period_days']
                    print(f"{planet.capitalize():<8}: {enhancement:+.2f}% opposition enhancement")
                    print(f"{'':>8}  Period: {period} days, Cycles: {cycles:.1f}")
                
                avg_enhancement = results.get('average_opposition_enhancement', 0)
                print(f"\nAverage opposition enhancement: {avg_enhancement:+.2f}%")
        
        # Temporal-spatial evolution details
        if results.get('analysis_type') == 'eclipse_temporal_spatial_evolution':
            print(f"Eclipse Date: {results.get('eclipse_date', 'Unknown')}")
            
            # Print eclipse progression summary
            if 'spatial_gradient_evolution' in results:
                evolution = results['spatial_gradient_evolution']
                if evolution:
                    print("\nECLIPSE PROGRESSION TIMELINE:")
                    print("-" * 50)
                    for point in evolution:
                        phase = point['eclipse_phase'].replace('_', ' ').title()
                        minutes = point['minutes_from_maximum']
                        gradient = point['spatial_gradient_percent']
                        print(f"{phase:<20} | {minutes:+6.0f} min | {gradient:+7.2f}% gradient")
            
            # Print zone evolution summary
            if 'eclipse_signature_by_zone' in results:
                signatures = results['eclipse_signature_by_zone']
                print("\nCOHERENCE EVOLUTION BY ECLIPSE ZONE:")
                print("-" * 45)
                for zone, evolution in signatures.items():
                    zone_name = zone.replace('_', ' ').title()
                    print(f"\n{zone_name}:")
                    for phase_data in evolution:
                        phase = phase_data['phase'].replace('_', ' ').title()
                        coherence = phase_data['mean_coherence']
                        n_meas = phase_data['n_measurements']
                        print(f"  {phase:<20}: {coherence:.6f} ({n_meas} measurements)")
            
            # Print station network coverage
            if 'temporal_evolution' in results:
                evolution = results['temporal_evolution']
                total_stations = set()
                for t in evolution:
                    total_stations.update(t.get('station_coherences', {}).keys())
                
                print(f"\nSTATION NETWORK COVERAGE:")
                print(f"Total unique stations tracked: {len(total_stations)}")
                print(f"Temporal evolution points: {len(evolution)}")
        
        elif results.get('analysis_type') == 'eclipse_differential_coherence':
            print(f"Eclipse Date: {results.get('eclipse_date', 'Unknown')}")
            signature = results.get('differential_signature', {})
            
            print("\nDIFFERENTIAL COHERENCE SIGNATURE:")
            print("-" * 50)
            
            for category, evolution in signature.items():
                cat_name = category.replace('_', ' ').title()
                print(f"\n{cat_name} Pairs:")
                for phase_data in evolution:
                    phase = phase_data['phase'].replace('_', ' ').title()
                    coherence = phase_data['mean_coherence']
                    n_pairs = phase_data['avg_n_pairs']
                    print(f"  {phase:<20}: {coherence:.6f} ({n_pairs} avg pairs)")

        # Spatial gradient details
        enhancement = results.get('enhancement_analysis', {})
        if enhancement:
            ratio = enhancement.get('enhancement_ratio', 1.0)
            percent = enhancement.get('enhancement_percent', 0.0)
            print(f"Enhancement Ratio: {ratio:.3f}x ({percent:+.1f}%)")
            print(f"Baseline Coherence: {enhancement.get('baseline_coherence', 0):.6f}")
            print(f"Eclipse Coherence: {enhancement.get('totality_coherence', 0):.6f}")
        
        # Save results
        output_dir = ROOT / "results" / "outputs"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save results - handle multi-center comprehensive analysis
        if args.event == 'comprehensive' and args.center == 'merged' and 'center_results' in results:
            # Save individual center results and combined results
            for center, center_results in results['center_results'].items():
                center_output_file = output_dir / f"step_10_comprehensive_high_res_{center}.json"
                try:
                    safe_json_write(center_results, center_output_file, indent=2)
                    print_status(f"Results saved for {center.upper()}: {center_output_file}", "SUCCESS")
                except Exception as e:
                    print_status(f"Failed to save results for {center}: {e}", "ERROR")
            
            # Save combined summary
            output_file = output_dir / f"step_10_comprehensive_multi_center_summary.json"
            try:
                safe_json_write(results, output_file, indent=2)
                print_status(f"Multi-center summary saved: {output_file}", "SUCCESS")
            except Exception as e:
                print_status(f"Failed to save multi-center summary: {e}", "ERROR")
        else:
            # Single file output for other cases
            # Create descriptive filename
            if args.event == 'all-astronomical':
                output_file = output_dir / f"step_10_comprehensive_astronomical_{args.center}.json"
            elif args.event == 'all-eclipses':
                output_file = output_dir / f"step_10_comprehensive_eclipses_{args.center}.json"
            elif args.event == 'differential' and args.eclipse_date != 'all':
                output_file = output_dir / f"step_10_eclipse_{args.eclipse_date}_{args.center}.json"
            else:
                output_file = output_dir / f"step_10_{args.event}_high_res_{args.center}.json"
            
            try:
                safe_json_write(results, output_file, indent=2)
                print_status(f"Results saved: {output_file}", "SUCCESS")
            except Exception as e:
                print_status(f"Failed to save results: {e}", "ERROR")
    else:
        print_status(f"Analysis failed: {results.get('error', 'Unknown error')}", "ERROR")
        return False
    
    elapsed_time = time.time() - start_time
    print(f"\nð STEP 10 COMPLETED in {elapsed_time:.1f} seconds")
    print("=" * 80)
    
    return results.get('success', False)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)




